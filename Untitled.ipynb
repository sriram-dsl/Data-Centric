{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1233a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/dsal/.local/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: langchain in /home/dsal/.local/lib/python3.8/site-packages (0.2.17)\n",
      "Requirement already satisfied: langchain-community in /home/dsal/.local/lib/python3.8/site-packages (0.2.19)\n",
      "Requirement already satisfied: faiss-cpu in /home/dsal/.local/lib/python3.8/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in /home/dsal/.local/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/dsal/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dsal/.local/lib/python3.8/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/dsal/.local/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/dsal/.local/lib/python3.8/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: packaging in /home/dsal/.local/lib/python3.8/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in /home/dsal/.local/lib/python3.8/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/dsal/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/dsal/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in /home/dsal/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dsal/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dsal/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/dsal/.local/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/dsal/.local/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dsal/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/dsal/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/dsal/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dsal/.local/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/dsal/.local/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dsal/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2022.9.24)\n",
      "Requirement already satisfied: greenlet>=1 in /home/dsal/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.8.6)\n",
      "Requirement already satisfied: jinja2 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/dsal/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/dsal/.local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers) (75.3.2)\n",
      "Requirement already satisfied: wheel in /home/dsal/.local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/dsal/.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers) (3.27.0)\n",
      "Requirement already satisfied: lit in /home/dsal/.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers) (16.0.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex!=2019.12.17 in /home/dsal/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/dsal/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dsal/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/dsal/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dsal/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: anyio in /home/dsal/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dsal/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dsal/.local/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/dsal/.local/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/dsal/.local/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dsal/.local/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dsal/.local/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dsal/.local/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/dsal/.local/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/dsal/.local/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas langchain langchain-community faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa349eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /home/dsal/.local/lib/python3.8/site-packages (0.2.17)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: faiss-cpu in /home/dsal/.local/lib/python3.8/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in /home/dsal/.local/lib/python3.8/site-packages (from langchain-ollama) (0.4.8)\n",
      "Requirement already satisfied: packaging in /home/dsal/.local/lib/python3.8/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/dsal/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/dsal/.local/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/dsal/.local/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (4.13.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dsal/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/dsal/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/dsal/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dsal/.local/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/dsal/.local/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dsal/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2022.9.24)\n",
      "Requirement already satisfied: greenlet>=1 in /home/dsal/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/dsal/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dsal/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dsal/.local/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/dsal/.local/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (2.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dsal/.local/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/dsal/.local/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/dsal/.local/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.1.1)\n",
      "Downloading langchain_ollama-0.1.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: langchain-ollama\n",
      "Successfully installed langchain-ollama-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-ollama faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79472dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"LD_LIBRARY_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30289b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with 1000 rows and 28 columns\n",
      "Columns: ['Customer_ID', 'Age', 'Gender', 'Income_Level', 'Marital_Status', 'Education_Level', 'Occupation', 'Location', 'Purchase_Category', 'Purchase_Amount', 'Frequency_of_Purchase', 'Purchase_Channel', 'Brand_Loyalty', 'Product_Rating', 'Time_Spent_on_Product_Research(hours)', 'Social_Media_Influence', 'Discount_Sensitivity', 'Return_Rate', 'Customer_Satisfaction', 'Engagement_with_Ads', 'Device_Used_for_Shopping', 'Payment_Method', 'Time_of_Purchase', 'Discount_Used', 'Customer_Loyalty_Program_Member', 'Purchase_Intent', 'Shipping_Preference', 'Time_to_Decision']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income_Level</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Location</th>\n",
       "      <th>Purchase_Category</th>\n",
       "      <th>Purchase_Amount</th>\n",
       "      <th>...</th>\n",
       "      <th>Customer_Satisfaction</th>\n",
       "      <th>Engagement_with_Ads</th>\n",
       "      <th>Device_Used_for_Shopping</th>\n",
       "      <th>Payment_Method</th>\n",
       "      <th>Time_of_Purchase</th>\n",
       "      <th>Discount_Used</th>\n",
       "      <th>Customer_Loyalty_Program_Member</th>\n",
       "      <th>Purchase_Intent</th>\n",
       "      <th>Shipping_Preference</th>\n",
       "      <th>Time_to_Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37-611-6911</td>\n",
       "      <td>22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Middle</td>\n",
       "      <td>Married</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Middle</td>\n",
       "      <td>Évry</td>\n",
       "      <td>Gardening &amp; Outdoors</td>\n",
       "      <td>$333.80</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>3/1/2024</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Need-based</td>\n",
       "      <td>No Preference</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29-392-9296</td>\n",
       "      <td>49</td>\n",
       "      <td>Male</td>\n",
       "      <td>High</td>\n",
       "      <td>Married</td>\n",
       "      <td>High School</td>\n",
       "      <td>High</td>\n",
       "      <td>Huocheng</td>\n",
       "      <td>Food &amp; Beverages</td>\n",
       "      <td>$222.22</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>High</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>4/16/2024</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Wants-based</td>\n",
       "      <td>Standard</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84-649-5117</td>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>Middle</td>\n",
       "      <td>Single</td>\n",
       "      <td>Master's</td>\n",
       "      <td>High</td>\n",
       "      <td>Huzhen</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>$426.22</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>Low</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>3/15/2024</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Impulsive</td>\n",
       "      <td>No Preference</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_ID  Age  Gender Income_Level Marital_Status Education_Level  \\\n",
       "0  37-611-6911   22  Female       Middle        Married      Bachelor's   \n",
       "1  29-392-9296   49    Male         High        Married     High School   \n",
       "2  84-649-5117   24  Female       Middle         Single        Master's   \n",
       "\n",
       "  Occupation  Location     Purchase_Category Purchase_Amount  ...  \\\n",
       "0     Middle      Évry  Gardening & Outdoors        $333.80   ...   \n",
       "1       High  Huocheng      Food & Beverages        $222.22   ...   \n",
       "2       High    Huzhen       Office Supplies        $426.22   ...   \n",
       "\n",
       "   Customer_Satisfaction Engagement_with_Ads  Device_Used_for_Shopping  \\\n",
       "0                      7                None                    Tablet   \n",
       "1                      5                High                    Tablet   \n",
       "2                      7                 Low                Smartphone   \n",
       "\n",
       "   Payment_Method  Time_of_Purchase Discount_Used  \\\n",
       "0     Credit Card          3/1/2024          True   \n",
       "1          PayPal         4/16/2024          True   \n",
       "2      Debit Card         3/15/2024          True   \n",
       "\n",
       "  Customer_Loyalty_Program_Member  Purchase_Intent  Shipping_Preference  \\\n",
       "0                           False       Need-based        No Preference   \n",
       "1                           False      Wants-based             Standard   \n",
       "2                            True        Impulsive        No Preference   \n",
       "\n",
       "  Time_to_Decision  \n",
       "0                2  \n",
       "1                6  \n",
       "2                3  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the CSV data\n",
    "csv_path = \"Ecommerce_Consumer_Behavior_Analysis_Data.csv\"  # Adjust path if needed\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Loaded CSV with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display a sample of the data\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4645bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time_of_Purchase column (first 5 values):\n",
      "0     3/1/2024\n",
      "1    4/16/2024\n",
      "2    3/15/2024\n",
      "3    10/4/2024\n",
      "4    1/30/2024\n",
      "Name: Time_of_Purchase, dtype: object\n",
      "\n",
      "Data type: object\n",
      "\n",
      "Date patterns found: {'##/#/####', '##/##/####', '#/##/####', '#/#/####'}\n"
     ]
    }
   ],
   "source": [
    "# Check the Time_of_Purchase column\n",
    "print(\"Time_of_Purchase column (first 5 values):\")\n",
    "print(df['Time_of_Purchase'].head(5))\n",
    "print(\"\\nData type:\", df['Time_of_Purchase'].dtype)\n",
    "\n",
    "# Check if the dates are in a consistent format\n",
    "import re\n",
    "date_patterns = set()\n",
    "for date in df['Time_of_Purchase'].head(20):\n",
    "    pattern = re.sub(r'[0-9]', '#', str(date))\n",
    "    date_patterns.add(pattern)\n",
    "print(\"\\nDate patterns found:\", date_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38272a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 1000 rows of e-commerce data\n",
      "\n",
      "Data types after preprocessing:\n",
      "Customer_ID                                      object\n",
      "Age                                               int64\n",
      "Gender                                           object\n",
      "Income_Level                                     object\n",
      "Marital_Status                                   object\n",
      "Education_Level                                  object\n",
      "Occupation                                       object\n",
      "Location                                         object\n",
      "Purchase_Category                                object\n",
      "Purchase_Amount                                 float64\n",
      "Frequency_of_Purchase                             int64\n",
      "Purchase_Channel                                 object\n",
      "Brand_Loyalty                                     int64\n",
      "Product_Rating                                    int64\n",
      "Time_Spent_on_Product_Research(hours)           float64\n",
      "Social_Media_Influence                           object\n",
      "Discount_Sensitivity                             object\n",
      "Return_Rate                                       int64\n",
      "Customer_Satisfaction                             int64\n",
      "Engagement_with_Ads                              object\n",
      "Device_Used_for_Shopping                         object\n",
      "Payment_Method                                   object\n",
      "Time_of_Purchase                         datetime64[ns]\n",
      "Discount_Used                                      bool\n",
      "Customer_Loyalty_Program_Member                    bool\n",
      "Purchase_Intent                                  object\n",
      "Shipping_Preference                              object\n",
      "Time_to_Decision                                  int64\n",
      "\n",
      "Sample of processed Purchase_Amount column:\n",
      "0    333.80\n",
      "1    222.22\n",
      "2    426.22\n",
      "3    101.31\n",
      "4    211.70\n",
      "Name: Purchase_Amount, dtype: float64\n",
      "\n",
      "Sample of processed Time_of_Purchase column:\n",
      "0   2024-03-01\n",
      "1   2024-04-16\n",
      "2   2024-03-15\n",
      "3   2024-10-04\n",
      "4   2024-01-30\n",
      "Name: Time_of_Purchase, dtype: datetime64[ns]\n",
      "\n",
      "Missing values:\n",
      "No missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1178015/511096090.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  processed_df['Purchase_Amount'] = processed_df['Purchase_Amount'].str.replace('$', '').str.strip().astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Updated data preprocessing function\n",
    "def preprocess_csv(df):\n",
    "    \"\"\"Clean and prepare the CSV data\"\"\"\n",
    "    # Create a copy to avoid warnings\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Clean string columns (remove whitespace)\n",
    "    for col in processed_df.select_dtypes(include=['object']).columns:\n",
    "        processed_df[col] = processed_df[col].str.strip() if hasattr(processed_df[col], 'str') else processed_df[col]\n",
    "    \n",
    "    # Convert Purchase Amount to numeric (remove $ symbol)\n",
    "    if 'Purchase_Amount' in processed_df.columns:\n",
    "        processed_df['Purchase_Amount'] = processed_df['Purchase_Amount'].str.replace('$', '').str.strip().astype(float)\n",
    "    \n",
    "    # Convert Time_of_Purchase to datetime format\n",
    "    if 'Time_of_Purchase' in processed_df.columns:\n",
    "        processed_df['Time_of_Purchase'] = pd.to_datetime(processed_df['Time_of_Purchase'], format='%m/%d/%Y', errors='coerce')\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Clean the data\n",
    "processed_df = preprocess_csv(df)\n",
    "print(f\"Preprocessed {len(processed_df)} rows of e-commerce data\")\n",
    "\n",
    "# Display data types after preprocessing\n",
    "print(\"\\nData types after preprocessing:\")\n",
    "print(processed_df.dtypes.to_string())\n",
    "\n",
    "# Display a sample of the preprocessed columns\n",
    "print(\"\\nSample of processed Purchase_Amount column:\")\n",
    "print(processed_df['Purchase_Amount'].head())\n",
    "\n",
    "print(\"\\nSample of processed Time_of_Purchase column:\")\n",
    "print(processed_df['Time_of_Purchase'].head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = processed_df.isnull().sum()\n",
    "print(\"\\nMissing values:\")\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192e050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document creation function framework defined.\n"
     ]
    }
   ],
   "source": [
    "# Step 5.1: Define the document creation function framework\n",
    "def create_table_rag_document(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive set of documents for Table RAG from the e-commerce dataset.\n",
    "    Returns both row-level documents and statistical segment documents.\n",
    "    \"\"\"\n",
    "    documents = []  # Will hold all our documents\n",
    "    \n",
    "    # Document types we'll create:\n",
    "    # 1. Row documents - one per customer\n",
    "    # 2. Single-dimension segment documents - statistics for one attribute\n",
    "    # 3. Multi-dimension segment documents - statistics for combinations\n",
    "    \n",
    "    # Helper function to format values consistently\n",
    "    def format_value(value):\n",
    "        \"\"\"Format different data types consistently for text embedding\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return \"N/A\"\n",
    "        elif isinstance(value, bool):\n",
    "            return str(value)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(type(value)):\n",
    "            return value.strftime('%Y-%m-%d')\n",
    "        elif isinstance(value, (int, float)):\n",
    "            if isinstance(value, float) and value == int(value):\n",
    "                return str(int(value))\n",
    "            return str(value)\n",
    "        else:\n",
    "            return str(value)\n",
    "    \n",
    "    # We'll implement the document creation in subsequent steps\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Initialize the function (we'll add implementation in next steps)\n",
    "print(\"Document creation function framework defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0152721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating row-level documents...\n",
      "Created 1000 row-level documents\n",
      "\n",
      "Sample row document:\n",
      "Customer data (Row 0):\n",
      "Demographics: Customer ID: 37-611-6911 | Age: 22 | Gender: Female | Income Level: Middle | Marital Status: Married | Education: Bachelor's | Occupation Level: Middle | Location: Évry\n",
      "Purchase: Category: Gardening & Outdoors | Amount: $333.80 | Frequency: 4 times | Channel: Mixed | Date: 2024-03-01 00:00:00\n",
      "Shopping Behavior: Brand Loyalty: 5/5 | Product Rating: 5/5 | Research Time: 2.0 hours | Social Media Influence: None | Discount Sensitivity: Somewhat Sensitive | Return Rate: 1 | Satisfaction: 7/10 | Ad Engagement: None | Used Discount: True | Loyalty Program Member: False | Purchase Intent: Need-based | Shipping Preference: No Preference | Time to Decision: 2 days\n",
      "Technology: Device: Tablet | Payment: Credit Card\n",
      "\n",
      "Metadata sample (first 5 keys):\n",
      "[('doc_type', 'customer_row'), ('row_idx', '0'), ('Customer_ID', '37-611-6911'), ('Age', '22'), ('Gender', 'Female')]\n"
     ]
    }
   ],
   "source": [
    "# Step 5.2: Implement row-level documents\n",
    "\n",
    "def create_table_rag_documents_row(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive set of documents for Table RAG from the e-commerce dataset.\n",
    "    Returns both row-level documents and statistical segment documents.\n",
    "    \"\"\"\n",
    "    documents = []  # Will hold all our documents\n",
    "    \n",
    "    # Helper function to format values consistently\n",
    "    def format_value(value):\n",
    "        \"\"\"Format different data types consistently for text embedding\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return \"N/A\"\n",
    "        elif isinstance(value, bool):\n",
    "            return str(value)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(type(value)):\n",
    "            return value.strftime('%Y-%m-%d')\n",
    "        elif isinstance(value, (int, float)):\n",
    "            if isinstance(value, float) and value == int(value):\n",
    "                return str(int(value))\n",
    "            return str(value)\n",
    "        else:\n",
    "            return str(value)\n",
    "    \n",
    "    # 1. Create row-level documents - one for each customer\n",
    "    print(\"Creating row-level documents...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create descriptive content for the document\n",
    "        content_parts = [f\"Customer data (Row {idx}):\"]\n",
    "        \n",
    "        # Demographic information\n",
    "        demographics = [\n",
    "            f\"Customer ID: {row['Customer_ID']}\",\n",
    "            f\"Age: {row['Age']}\",\n",
    "            f\"Gender: {row['Gender']}\",\n",
    "            f\"Income Level: {row['Income_Level']}\",\n",
    "            f\"Marital Status: {row['Marital_Status']}\",\n",
    "            f\"Education: {row['Education_Level']}\",\n",
    "            f\"Occupation Level: {row['Occupation']}\",\n",
    "            f\"Location: {row['Location']}\"\n",
    "        ]\n",
    "        content_parts.append(\"Demographics: \" + \" | \".join(demographics))\n",
    "        \n",
    "        # Purchase information\n",
    "        purchase = [\n",
    "            f\"Category: {row['Purchase_Category']}\",\n",
    "            f\"Amount: ${row['Purchase_Amount']:.2f}\",\n",
    "            f\"Frequency: {row['Frequency_of_Purchase']} times\",\n",
    "            f\"Channel: {row['Purchase_Channel']}\",\n",
    "            f\"Date: {format_value(row['Time_of_Purchase'])}\"\n",
    "        ]\n",
    "        content_parts.append(\"Purchase: \" + \" | \".join(purchase))\n",
    "        \n",
    "        # Customer behavior\n",
    "        behavior = [\n",
    "            f\"Brand Loyalty: {row['Brand_Loyalty']}/5\",\n",
    "            f\"Product Rating: {row['Product_Rating']}/5\",\n",
    "            f\"Research Time: {row['Time_Spent_on_Product_Research(hours)']} hours\",\n",
    "            f\"Social Media Influence: {row['Social_Media_Influence']}\",\n",
    "            f\"Discount Sensitivity: {row['Discount_Sensitivity']}\",\n",
    "            f\"Return Rate: {row['Return_Rate']}\",\n",
    "            f\"Satisfaction: {row['Customer_Satisfaction']}/10\",\n",
    "            f\"Ad Engagement: {row['Engagement_with_Ads']}\",\n",
    "            f\"Used Discount: {row['Discount_Used']}\",\n",
    "            f\"Loyalty Program Member: {row['Customer_Loyalty_Program_Member']}\",\n",
    "            f\"Purchase Intent: {row['Purchase_Intent']}\",\n",
    "            f\"Shipping Preference: {row['Shipping_Preference']}\",\n",
    "            f\"Time to Decision: {row['Time_to_Decision']} days\"\n",
    "        ]\n",
    "        content_parts.append(\"Shopping Behavior: \" + \" | \".join(behavior))\n",
    "        \n",
    "        # Device and payment\n",
    "        tech = [\n",
    "            f\"Device: {row['Device_Used_for_Shopping']}\",\n",
    "            f\"Payment: {row['Payment_Method']}\"\n",
    "        ]\n",
    "        content_parts.append(\"Technology: \" + \" | \".join(tech))\n",
    "        \n",
    "        # Join all parts into the final content\n",
    "        content = \"\\n\".join(content_parts)\n",
    "        \n",
    "        # Create metadata with all fields\n",
    "        metadata = {\n",
    "            'doc_type': 'customer_row',\n",
    "            'row_idx': str(idx)\n",
    "        }\n",
    "        \n",
    "        # Add all columns to metadata for filtering\n",
    "        for col in df.columns:\n",
    "            metadata[col] = format_value(row[col])\n",
    "        \n",
    "        # Create the document\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "    # We'll add segment documents in subsequent steps\n",
    "    \n",
    "    print(f\"Created {len(documents)} row-level documents\")\n",
    "    return documents\n",
    "\n",
    "# Create row-level documents\n",
    "row_documents = create_table_rag_documents_row(processed_df)\n",
    "\n",
    "# Display a sample row document\n",
    "print(\"\\nSample row document:\")\n",
    "print(row_documents[0].page_content)\n",
    "print(\"\\nMetadata sample (first 5 keys):\")\n",
    "print(list(row_documents[0].metadata.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc38f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating row-level documents...\n",
      "\n",
      "Creating single-dimension segment statistics...\n",
      "Created 67 single-dimension segment documents\n",
      "Total documents created: 1067\n",
      "\n",
      "Sample segment document:\n",
      "Segment Analysis: Gender: Female\n",
      "Total customers in this segment: 452 (45.2% of all customers)\n",
      "Purchase metrics:\n",
      "- Average purchase amount: $282.09\n",
      "- Total purchase amount: $127503.71\n",
      "- Average customer satisfaction: 5.3/10\n",
      "Customer profile:\n",
      "- Discount usage rate: 53.1%\n",
      "- Loyalty program membership: 49.1%\n",
      "Purchase channel distribution:\n",
      "- Mixed: 34.1%\n",
      "- Online: 34.1%\n",
      "- In-Store: 31.9%\n",
      "Top product categories:\n",
      "- Jewelry & Accessories: 6.4%\n",
      "- Electronics: 6.2%\n",
      "- Toys & Games: 5.3%\n",
      "- Sports & Outdoors: 5.1%\n",
      "- Health Care: 4.9%\n",
      "Device usage:\n",
      "- Desktop: 35.6%\n",
      "- Smartphone: 32.5%\n",
      "- Tablet: 31.9%\n",
      "\n",
      "Segment metadata:\n",
      "{'doc_type': 'segment_statistics', 'dimension': 'Gender', 'segment_value': 'Female', 'segment_name': 'Gender: Female', 'count': '452', 'percentage': '45.2%', 'avg_purchase': '$282.09', 'total_purchase': '$127503.71', 'avg_satisfaction': '5.3', 'discount_usage': '53.1%', 'loyalty_membership': '49.1%'}\n"
     ]
    }
   ],
   "source": [
    "# Step 5.3: Implement single-dimension segment statistics\n",
    "\n",
    "def create_table_rag_documents_singledim(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive set of documents for Table RAG from the e-commerce dataset.\n",
    "    Returns both row-level documents and statistical segment documents.\n",
    "    \"\"\"\n",
    "    documents = []  # Will hold all our documents\n",
    "    \n",
    "    # Helper function to format values consistently\n",
    "    def format_value(value):\n",
    "        \"\"\"Format different data types consistently for text embedding\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return \"N/A\"\n",
    "        elif isinstance(value, bool):\n",
    "            return str(value)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(type(value)):\n",
    "            return value.strftime('%Y-%m-%d')\n",
    "        elif isinstance(value, (int, float)):\n",
    "            if isinstance(value, float) and value == int(value):\n",
    "                return str(int(value))\n",
    "            return str(value)\n",
    "        else:\n",
    "            return str(value)\n",
    "    \n",
    "    # 1. Create row-level documents - one for each customer\n",
    "    print(\"Creating row-level documents...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create descriptive content for the document\n",
    "        content_parts = [f\"Customer data (Row {idx}):\"]\n",
    "        \n",
    "        # Demographic information\n",
    "        demographics = [\n",
    "            f\"Customer ID: {row['Customer_ID']}\",\n",
    "            f\"Age: {row['Age']}\",\n",
    "            f\"Gender: {row['Gender']}\",\n",
    "            f\"Income Level: {row['Income_Level']}\",\n",
    "            f\"Marital Status: {row['Marital_Status']}\",\n",
    "            f\"Education: {row['Education_Level']}\",\n",
    "            f\"Occupation Level: {row['Occupation']}\",\n",
    "            f\"Location: {row['Location']}\"\n",
    "        ]\n",
    "        content_parts.append(\"Demographics: \" + \" | \".join(demographics))\n",
    "        \n",
    "        # Purchase information\n",
    "        purchase = [\n",
    "            f\"Category: {row['Purchase_Category']}\",\n",
    "            f\"Amount: ${row['Purchase_Amount']:.2f}\",\n",
    "            f\"Frequency: {row['Frequency_of_Purchase']} times\",\n",
    "            f\"Channel: {row['Purchase_Channel']}\",\n",
    "            f\"Date: {format_value(row['Time_of_Purchase'])}\"\n",
    "        ]\n",
    "        content_parts.append(\"Purchase: \" + \" | \".join(purchase))\n",
    "        \n",
    "        # Customer behavior\n",
    "        behavior = [\n",
    "            f\"Brand Loyalty: {row['Brand_Loyalty']}/5\",\n",
    "            f\"Product Rating: {row['Product_Rating']}/5\",\n",
    "            f\"Research Time: {row['Time_Spent_on_Product_Research(hours)']} hours\",\n",
    "            f\"Social Media Influence: {row['Social_Media_Influence']}\",\n",
    "            f\"Discount Sensitivity: {row['Discount_Sensitivity']}\",\n",
    "            f\"Return Rate: {row['Return_Rate']}\",\n",
    "            f\"Satisfaction: {row['Customer_Satisfaction']}/10\",\n",
    "            f\"Ad Engagement: {row['Engagement_with_Ads']}\",\n",
    "            f\"Used Discount: {row['Discount_Used']}\",\n",
    "            f\"Loyalty Program Member: {row['Customer_Loyalty_Program_Member']}\",\n",
    "            f\"Purchase Intent: {row['Purchase_Intent']}\",\n",
    "            f\"Shipping Preference: {row['Shipping_Preference']}\",\n",
    "            f\"Time to Decision: {row['Time_to_Decision']} days\"\n",
    "        ]\n",
    "        content_parts.append(\"Shopping Behavior: \" + \" | \".join(behavior))\n",
    "        \n",
    "        # Device and payment\n",
    "        tech = [\n",
    "            f\"Device: {row['Device_Used_for_Shopping']}\",\n",
    "            f\"Payment: {row['Payment_Method']}\"\n",
    "        ]\n",
    "        content_parts.append(\"Technology: \" + \" | \".join(tech))\n",
    "        \n",
    "        # Join all parts into the final content\n",
    "        content = \"\\n\".join(content_parts)\n",
    "        \n",
    "        # Create metadata with all fields\n",
    "        metadata = {\n",
    "            'doc_type': 'customer_row',\n",
    "            'row_idx': str(idx)\n",
    "        }\n",
    "        \n",
    "        # Add all columns to metadata for filtering\n",
    "        for col in df.columns:\n",
    "            metadata[col] = format_value(row[col])\n",
    "        \n",
    "        # Create the document\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "    # 2. Create single-dimension segment statistics\n",
    "    print(\"\\nCreating single-dimension segment statistics...\")\n",
    "    \n",
    "    # Define key dimensions for segmentation\n",
    "    single_dimensions = [\n",
    "        # Demographics\n",
    "        {'column': 'Gender', 'name': 'Gender'},\n",
    "        {'column': 'Income_Level', 'name': 'Income Level'},\n",
    "        {'column': 'Marital_Status', 'name': 'Marital Status'},\n",
    "        {'column': 'Education_Level', 'name': 'Education Level'},\n",
    "        \n",
    "        # Purchase related\n",
    "        {'column': 'Purchase_Category', 'name': 'Product Category'},\n",
    "        {'column': 'Purchase_Channel', 'name': 'Purchase Channel'},\n",
    "        {'column': 'Device_Used_for_Shopping', 'name': 'Device'},\n",
    "        {'column': 'Payment_Method', 'name': 'Payment Method'},\n",
    "        \n",
    "        # Customer behavior\n",
    "        {'column': 'Discount_Used', 'name': 'Discount Usage'},\n",
    "        {'column': 'Customer_Loyalty_Program_Member', 'name': 'Loyalty Program'},\n",
    "        {'column': 'Purchase_Intent', 'name': 'Purchase Intent'},\n",
    "        {'column': 'Social_Media_Influence', 'name': 'Social Media Influence'}\n",
    "    ]\n",
    "    \n",
    "    # Add age groups as a special case\n",
    "    age_groups = [\n",
    "        {'min': 0, 'max': 25, 'label': '18-25'},\n",
    "        {'min': 26, 'max': 35, 'label': '26-35'},\n",
    "        {'min': 36, 'max': 50, 'label': '36-50'},\n",
    "        {'min': 51, 'max': 100, 'label': '51+'}\n",
    "    ]\n",
    "    \n",
    "    segment_count = 0\n",
    "    \n",
    "    # Process each dimension\n",
    "    for dim in single_dimensions:\n",
    "        col = dim['column']\n",
    "        name = dim['name']\n",
    "        \n",
    "        # Get unique values for this dimension\n",
    "        unique_values = df[col].unique()\n",
    "        \n",
    "        for value in unique_values:\n",
    "            # Filter data for this segment\n",
    "            segment_data = df[df[col] == value]\n",
    "            \n",
    "            if len(segment_data) == 0:\n",
    "                continue  # Skip empty segments\n",
    "                \n",
    "            # Calculate key statistics for this segment\n",
    "            stats = {\n",
    "                'count': len(segment_data),\n",
    "                'total_count': len(df),  # Total in the dataset\n",
    "                'percentage': len(segment_data) / len(df) * 100,\n",
    "                'avg_purchase': segment_data['Purchase_Amount'].mean(),\n",
    "                'total_purchase': segment_data['Purchase_Amount'].sum(),\n",
    "                'avg_satisfaction': segment_data['Customer_Satisfaction'].mean(),\n",
    "                'discount_usage': segment_data['Discount_Used'].mean() * 100,\n",
    "                'loyalty_membership': segment_data['Customer_Loyalty_Program_Member'].mean() * 100\n",
    "            }\n",
    "            \n",
    "            # Create a descriptive title for this segment\n",
    "            segment_title = f\"{name}: {value}\"\n",
    "            \n",
    "            # Create detailed content for this segment\n",
    "            content_parts = [\n",
    "                f\"Segment Analysis: {segment_title}\",\n",
    "                f\"Total customers in this segment: {stats['count']} ({stats['percentage']:.1f}% of all customers)\",\n",
    "                f\"Purchase metrics:\",\n",
    "                f\"- Average purchase amount: ${stats['avg_purchase']:.2f}\",\n",
    "                f\"- Total purchase amount: ${stats['total_purchase']:.2f}\",\n",
    "                f\"- Average customer satisfaction: {stats['avg_satisfaction']:.1f}/10\",\n",
    "                f\"Customer profile:\",\n",
    "                f\"- Discount usage rate: {stats['discount_usage']:.1f}%\",\n",
    "                f\"- Loyalty program membership: {stats['loyalty_membership']:.1f}%\"\n",
    "            ]\n",
    "            \n",
    "            # Add distribution of other key dimensions\n",
    "            if col != 'Purchase_Channel':\n",
    "                channel_dist = segment_data['Purchase_Channel'].value_counts(normalize=True) * 100\n",
    "                content_parts.append(\"Purchase channel distribution:\")\n",
    "                for channel, pct in channel_dist.items():\n",
    "                    content_parts.append(f\"- {channel}: {pct:.1f}%\")\n",
    "            \n",
    "            if col != 'Purchase_Category':\n",
    "                top_categories = segment_data['Purchase_Category'].value_counts(normalize=True).head(5) * 100\n",
    "                content_parts.append(\"Top product categories:\")\n",
    "                for category, pct in top_categories.items():\n",
    "                    content_parts.append(f\"- {category}: {pct:.1f}%\")\n",
    "            \n",
    "            if col != 'Device_Used_for_Shopping':\n",
    "                device_dist = segment_data['Device_Used_for_Shopping'].value_counts(normalize=True) * 100\n",
    "                content_parts.append(\"Device usage:\")\n",
    "                for device, pct in device_dist.items():\n",
    "                    content_parts.append(f\"- {device}: {pct:.1f}%\")\n",
    "            \n",
    "            # Join all parts into the final content\n",
    "            content = \"\\n\".join(content_parts)\n",
    "            \n",
    "            # Create metadata for this segment\n",
    "            metadata = {\n",
    "                'doc_type': 'segment_statistics',\n",
    "                'dimension': col,\n",
    "                'segment_value': str(value),\n",
    "                'segment_name': segment_title,\n",
    "                'count': str(stats['count']),\n",
    "                'percentage': f\"{stats['percentage']:.1f}%\",\n",
    "                'avg_purchase': f\"${stats['avg_purchase']:.2f}\",\n",
    "                'total_purchase': f\"${stats['total_purchase']:.2f}\",\n",
    "                'avg_satisfaction': f\"{stats['avg_satisfaction']:.1f}\",\n",
    "                'discount_usage': f\"{stats['discount_usage']:.1f}%\",\n",
    "                'loyalty_membership': f\"{stats['loyalty_membership']:.1f}%\"\n",
    "            }\n",
    "            \n",
    "            # Create the document\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "            segment_count += 1\n",
    "    \n",
    "    # Process age groups as a special case\n",
    "    for group in age_groups:\n",
    "        # Filter data for this age group\n",
    "        if group['min'] == 0:\n",
    "            segment_data = df[df['Age'] <= group['max']]\n",
    "        else:\n",
    "            segment_data = df[(df['Age'] >= group['min']) & (df['Age'] <= group['max'])]\n",
    "        \n",
    "        if len(segment_data) == 0:\n",
    "            continue  # Skip empty segments\n",
    "            \n",
    "        # Calculate key statistics for this segment\n",
    "        stats = {\n",
    "            'count': len(segment_data),\n",
    "            'total_count': len(df),  # Total in the dataset\n",
    "            'percentage': len(segment_data) / len(df) * 100,\n",
    "            'avg_purchase': segment_data['Purchase_Amount'].mean(),\n",
    "            'total_purchase': segment_data['Purchase_Amount'].sum(),\n",
    "            'avg_satisfaction': segment_data['Customer_Satisfaction'].mean(),\n",
    "            'discount_usage': segment_data['Discount_Used'].mean() * 100,\n",
    "            'loyalty_membership': segment_data['Customer_Loyalty_Program_Member'].mean() * 100\n",
    "        }\n",
    "        \n",
    "        # Create a descriptive title for this segment\n",
    "        segment_title = f\"Age Group: {group['label']}\"\n",
    "        \n",
    "        # Create detailed content for this segment\n",
    "        content_parts = [\n",
    "            f\"Segment Analysis: {segment_title}\",\n",
    "            f\"Total customers in this segment: {stats['count']} ({stats['percentage']:.1f}% of all customers)\",\n",
    "            f\"Purchase metrics:\",\n",
    "            f\"- Average purchase amount: ${stats['avg_purchase']:.2f}\",\n",
    "            f\"- Total purchase amount: ${stats['total_purchase']:.2f}\",\n",
    "            f\"- Average customer satisfaction: {stats['avg_satisfaction']:.1f}/10\",\n",
    "            f\"Customer profile:\",\n",
    "            f\"- Discount usage rate: {stats['discount_usage']:.1f}%\",\n",
    "            f\"- Loyalty program membership: {stats['loyalty_membership']:.1f}%\"\n",
    "        ]\n",
    "        \n",
    "        # Add distribution of other key dimensions\n",
    "        channel_dist = segment_data['Purchase_Channel'].value_counts(normalize=True) * 100\n",
    "        content_parts.append(\"Purchase channel distribution:\")\n",
    "        for channel, pct in channel_dist.items():\n",
    "            content_parts.append(f\"- {channel}: {pct:.1f}%\")\n",
    "        \n",
    "        top_categories = segment_data['Purchase_Category'].value_counts(normalize=True).head(5) * 100\n",
    "        content_parts.append(\"Top product categories:\")\n",
    "        for category, pct in top_categories.items():\n",
    "            content_parts.append(f\"- {category}: {pct:.1f}%\")\n",
    "        \n",
    "        device_dist = segment_data['Device_Used_for_Shopping'].value_counts(normalize=True) * 100\n",
    "        content_parts.append(\"Device usage:\")\n",
    "        for device, pct in device_dist.items():\n",
    "            content_parts.append(f\"- {device}: {pct:.1f}%\")\n",
    "        \n",
    "        # Join all parts into the final content\n",
    "        content = \"\\n\".join(content_parts)\n",
    "        \n",
    "        # Create metadata for this segment\n",
    "        metadata = {\n",
    "            'doc_type': 'segment_statistics',\n",
    "            'dimension': 'Age_Group',\n",
    "            'segment_value': group['label'],\n",
    "            'segment_name': segment_title,\n",
    "            'count': str(stats['count']),\n",
    "            'percentage': f\"{stats['percentage']:.1f}%\",\n",
    "            'avg_purchase': f\"${stats['avg_purchase']:.2f}\",\n",
    "            'total_purchase': f\"${stats['total_purchase']:.2f}\",\n",
    "            'avg_satisfaction': f\"{stats['avg_satisfaction']:.1f}\",\n",
    "            'discount_usage': f\"{stats['discount_usage']:.1f}%\",\n",
    "            'loyalty_membership': f\"{stats['loyalty_membership']:.1f}%\"\n",
    "        }\n",
    "        \n",
    "        # Create the document\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "        segment_count += 1\n",
    "    \n",
    "    print(f\"Created {segment_count} single-dimension segment documents\")\n",
    "    print(f\"Total documents created: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Create documents including single-dimension segments\n",
    "single_dim_documents = create_table_rag_documents_singledim(processed_df)\n",
    "\n",
    "# Display a sample segment document\n",
    "segment_docs = [doc for doc in single_dim_documents if doc.metadata['doc_type'] == 'segment_statistics']\n",
    "if segment_docs:\n",
    "    print(\"\\nSample segment document:\")\n",
    "    print(segment_docs[0].page_content)\n",
    "    print(\"\\nSegment metadata:\")\n",
    "    print(segment_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1684b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating row-level documents...\n",
      "\n",
      "Creating single-dimension segment statistics...\n",
      "Created 67 single-dimension segment documents\n",
      "\n",
      "Creating multi-dimension segment statistics...\n",
      "Created 288 multi-dimension segment documents\n",
      "Total documents created: 1355\n",
      "\n",
      "Sample multi-dimension segment document:\n",
      "Multi-Dimension Segment Analysis: Product Category: Electronics + Discount Usage: False\n",
      "Customer counts:\n",
      "- Total in this segment: 28\n",
      "- Percentage of Electronics product category: 51.9%\n",
      "- Percentage of all customers: 2.8%\n",
      "Purchase metrics:\n",
      "- Average purchase amount: $255.51\n",
      "- Total purchase amount: $7154.36\n",
      "- Average customer satisfaction: 5.5/10\n",
      "Customer profile:\n",
      "- Discount usage rate: 0.0%\n",
      "- Loyalty program membership: 60.7%\n",
      "Gender distribution:\n",
      "- Female: 46.4%\n",
      "- Male: 39.3%\n",
      "- Non-binary: 3.6%\n",
      "- Genderfluid: 3.6%\n",
      "- Bigender: 3.6%\n",
      "- Polygender: 3.6%\n",
      "Insight: Most Electronics product category customers (51.9%) are False discount usage customers.\n",
      "\n",
      "Multi-segment metadata:\n",
      "{'doc_type': 'multi_segment_statistics', 'dimension1': 'Gender', 'dimension2': 'Purchase_Channel', 'value1': 'Female', 'value2': 'Mixed', 'segment_name': 'Gender: Female + Purchase Channel: Mixed', 'count': '154', 'parent_count': '452', 'percentage_of_parent': '34.1%', 'percentage_of_total': '15.4%', 'avg_purchase': '$282.35', 'total_purchase': '$43481.99', 'avg_satisfaction': '5.4', 'discount_usage': '53.9%', 'loyalty_membership': '53.9%'}\n"
     ]
    }
   ],
   "source": [
    "# Step 5.4: Implement multi-dimension segment statistics\n",
    "\n",
    "def create_table_rag_documents_multidim(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive set of documents for Table RAG from the e-commerce dataset.\n",
    "    Returns row-level documents, single-dimension and multi-dimension segment statistics.\n",
    "    \"\"\"\n",
    "    documents = []  # Will hold all our documents\n",
    "    \n",
    "    # Helper function to format values consistently\n",
    "    def format_value(value):\n",
    "        \"\"\"Format different data types consistently for text embedding\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return \"N/A\"\n",
    "        elif isinstance(value, bool):\n",
    "            return str(value)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(type(value)):\n",
    "            return value.strftime('%Y-%m-%d')\n",
    "        elif isinstance(value, (int, float)):\n",
    "            if isinstance(value, float) and value == int(value):\n",
    "                return str(int(value))\n",
    "            return str(value)\n",
    "        else:\n",
    "            return str(value)\n",
    "    \n",
    "    # 1. Create row-level documents - one for each customer\n",
    "    print(\"Creating row-level documents...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create descriptive content for the document\n",
    "        content_parts = [f\"Customer data (Row {idx}):\"]\n",
    "        \n",
    "        # Demographic information\n",
    "        demographics = [\n",
    "            f\"Customer ID: {row['Customer_ID']}\",\n",
    "            f\"Age: {row['Age']}\",\n",
    "            f\"Gender: {row['Gender']}\",\n",
    "            f\"Income Level: {row['Income_Level']}\",\n",
    "            f\"Marital Status: {row['Marital_Status']}\",\n",
    "            f\"Education: {row['Education_Level']}\",\n",
    "            f\"Occupation Level: {row['Occupation']}\",\n",
    "            f\"Location: {row['Location']}\"\n",
    "        ]\n",
    "        content_parts.append(\"Demographics: \" + \" | \".join(demographics))\n",
    "        \n",
    "        # Purchase information\n",
    "        purchase = [\n",
    "            f\"Category: {row['Purchase_Category']}\",\n",
    "            f\"Amount: ${row['Purchase_Amount']:.2f}\",\n",
    "            f\"Frequency: {row['Frequency_of_Purchase']} times\",\n",
    "            f\"Channel: {row['Purchase_Channel']}\",\n",
    "            f\"Date: {format_value(row['Time_of_Purchase'])}\"\n",
    "        ]\n",
    "        content_parts.append(\"Purchase: \" + \" | \".join(purchase))\n",
    "        \n",
    "        # Customer behavior\n",
    "        behavior = [\n",
    "            f\"Brand Loyalty: {row['Brand_Loyalty']}/5\",\n",
    "            f\"Product Rating: {row['Product_Rating']}/5\",\n",
    "            f\"Research Time: {row['Time_Spent_on_Product_Research(hours)']} hours\",\n",
    "            f\"Social Media Influence: {row['Social_Media_Influence']}\",\n",
    "            f\"Discount Sensitivity: {row['Discount_Sensitivity']}\",\n",
    "            f\"Return Rate: {row['Return_Rate']}\",\n",
    "            f\"Satisfaction: {row['Customer_Satisfaction']}/10\",\n",
    "            f\"Ad Engagement: {row['Engagement_with_Ads']}\",\n",
    "            f\"Used Discount: {row['Discount_Used']}\",\n",
    "            f\"Loyalty Program Member: {row['Customer_Loyalty_Program_Member']}\",\n",
    "            f\"Purchase Intent: {row['Purchase_Intent']}\",\n",
    "            f\"Shipping Preference: {row['Shipping_Preference']}\",\n",
    "            f\"Time to Decision: {row['Time_to_Decision']} days\"\n",
    "        ]\n",
    "        content_parts.append(\"Shopping Behavior: \" + \" | \".join(behavior))\n",
    "        \n",
    "        # Device and payment\n",
    "        tech = [\n",
    "            f\"Device: {row['Device_Used_for_Shopping']}\",\n",
    "            f\"Payment: {row['Payment_Method']}\"\n",
    "        ]\n",
    "        content_parts.append(\"Technology: \" + \" | \".join(tech))\n",
    "        \n",
    "        # Join all parts into the final content\n",
    "        content = \"\\n\".join(content_parts)\n",
    "        \n",
    "        # Create metadata with all fields\n",
    "        metadata = {\n",
    "            'doc_type': 'customer_row',\n",
    "            'row_idx': str(idx)\n",
    "        }\n",
    "        \n",
    "        # Add all columns to metadata for filtering\n",
    "        for col in df.columns:\n",
    "            metadata[col] = format_value(row[col])\n",
    "        \n",
    "        # Create the document\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "    # 2. Create single-dimension segment statistics\n",
    "    print(\"\\nCreating single-dimension segment statistics...\")\n",
    "    \n",
    "    # Define key dimensions for segmentation\n",
    "    single_dimensions = [\n",
    "        # Demographics\n",
    "        {'column': 'Gender', 'name': 'Gender'},\n",
    "        {'column': 'Income_Level', 'name': 'Income Level'},\n",
    "        {'column': 'Marital_Status', 'name': 'Marital Status'},\n",
    "        {'column': 'Education_Level', 'name': 'Education Level'},\n",
    "        \n",
    "        # Purchase related\n",
    "        {'column': 'Purchase_Category', 'name': 'Product Category'},\n",
    "        {'column': 'Purchase_Channel', 'name': 'Purchase Channel'},\n",
    "        {'column': 'Device_Used_for_Shopping', 'name': 'Device'},\n",
    "        {'column': 'Payment_Method', 'name': 'Payment Method'},\n",
    "        \n",
    "        # Customer behavior\n",
    "        {'column': 'Discount_Used', 'name': 'Discount Usage'},\n",
    "        {'column': 'Customer_Loyalty_Program_Member', 'name': 'Loyalty Program'},\n",
    "        {'column': 'Purchase_Intent', 'name': 'Purchase Intent'},\n",
    "        {'column': 'Social_Media_Influence', 'name': 'Social Media Influence'}\n",
    "    ]\n",
    "    \n",
    "    # Add age groups as a special case\n",
    "    age_groups = [\n",
    "        {'min': 0, 'max': 25, 'label': '18-25'},\n",
    "        {'min': 26, 'max': 35, 'label': '26-35'},\n",
    "        {'min': 36, 'max': 50, 'label': '36-50'},\n",
    "        {'min': 51, 'max': 100, 'label': '51+'}\n",
    "    ]\n",
    "    \n",
    "    segment_count = 0\n",
    "    \n",
    "    # Process each dimension\n",
    "    for dim in single_dimensions:\n",
    "        col = dim['column']\n",
    "        name = dim['name']\n",
    "        \n",
    "        # Get unique values for this dimension\n",
    "        unique_values = df[col].unique()\n",
    "        \n",
    "        for value in unique_values:\n",
    "            # Filter data for this segment\n",
    "            segment_data = df[df[col] == value]\n",
    "            \n",
    "            if len(segment_data) == 0:\n",
    "                continue  # Skip empty segments\n",
    "                \n",
    "            # Calculate key statistics for this segment\n",
    "            stats = {\n",
    "                'count': len(segment_data),\n",
    "                'total_count': len(df),  # Total in the dataset\n",
    "                'percentage': len(segment_data) / len(df) * 100,\n",
    "                'avg_purchase': segment_data['Purchase_Amount'].mean(),\n",
    "                'total_purchase': segment_data['Purchase_Amount'].sum(),\n",
    "                'avg_satisfaction': segment_data['Customer_Satisfaction'].mean(),\n",
    "                'discount_usage': segment_data['Discount_Used'].mean() * 100,\n",
    "                'loyalty_membership': segment_data['Customer_Loyalty_Program_Member'].mean() * 100\n",
    "            }\n",
    "            \n",
    "            # Create a descriptive title for this segment\n",
    "            segment_title = f\"{name}: {value}\"\n",
    "            \n",
    "            # Create detailed content for this segment\n",
    "            content_parts = [\n",
    "                f\"Segment Analysis: {segment_title}\",\n",
    "                f\"Total customers in this segment: {stats['count']} ({stats['percentage']:.1f}% of all customers)\",\n",
    "                f\"Purchase metrics:\",\n",
    "                f\"- Average purchase amount: ${stats['avg_purchase']:.2f}\",\n",
    "                f\"- Total purchase amount: ${stats['total_purchase']:.2f}\",\n",
    "                f\"- Average customer satisfaction: {stats['avg_satisfaction']:.1f}/10\",\n",
    "                f\"Customer profile:\",\n",
    "                f\"- Discount usage rate: {stats['discount_usage']:.1f}%\",\n",
    "                f\"- Loyalty program membership: {stats['loyalty_membership']:.1f}%\"\n",
    "            ]\n",
    "            \n",
    "            # Add distribution of other key dimensions\n",
    "            if col != 'Purchase_Channel':\n",
    "                channel_dist = segment_data['Purchase_Channel'].value_counts(normalize=True) * 100\n",
    "                content_parts.append(\"Purchase channel distribution:\")\n",
    "                for channel, pct in channel_dist.items():\n",
    "                    content_parts.append(f\"- {channel}: {pct:.1f}%\")\n",
    "            \n",
    "            if col != 'Purchase_Category':\n",
    "                top_categories = segment_data['Purchase_Category'].value_counts(normalize=True).head(5) * 100\n",
    "                content_parts.append(\"Top product categories:\")\n",
    "                for category, pct in top_categories.items():\n",
    "                    content_parts.append(f\"- {category}: {pct:.1f}%\")\n",
    "            \n",
    "            if col != 'Device_Used_for_Shopping':\n",
    "                device_dist = segment_data['Device_Used_for_Shopping'].value_counts(normalize=True) * 100\n",
    "                content_parts.append(\"Device usage:\")\n",
    "                for device, pct in device_dist.items():\n",
    "                    content_parts.append(f\"- {device}: {pct:.1f}%\")\n",
    "            \n",
    "            # Join all parts into the final content\n",
    "            content = \"\\n\".join(content_parts)\n",
    "            \n",
    "            # Create metadata for this segment\n",
    "            metadata = {\n",
    "                'doc_type': 'segment_statistics',\n",
    "                'dimension': col,\n",
    "                'segment_value': str(value),\n",
    "                'segment_name': segment_title,\n",
    "                'count': str(stats['count']),\n",
    "                'percentage': f\"{stats['percentage']:.1f}%\",\n",
    "                'avg_purchase': f\"${stats['avg_purchase']:.2f}\",\n",
    "                'total_purchase': f\"${stats['total_purchase']:.2f}\",\n",
    "                'avg_satisfaction': f\"{stats['avg_satisfaction']:.1f}\",\n",
    "                'discount_usage': f\"{stats['discount_usage']:.1f}%\",\n",
    "                'loyalty_membership': f\"{stats['loyalty_membership']:.1f}%\"\n",
    "            }\n",
    "            \n",
    "            # Create the document\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "            segment_count += 1\n",
    "    \n",
    "    # Process age groups as a special case\n",
    "    for group in age_groups:\n",
    "        # Filter data for this age group\n",
    "        if group['min'] == 0:\n",
    "            segment_data = df[df['Age'] <= group['max']]\n",
    "        else:\n",
    "            segment_data = df[(df['Age'] >= group['min']) & (df['Age'] <= group['max'])]\n",
    "        \n",
    "        if len(segment_data) == 0:\n",
    "            continue  # Skip empty segments\n",
    "            \n",
    "        # Calculate key statistics for this segment\n",
    "        stats = {\n",
    "            'count': len(segment_data),\n",
    "            'total_count': len(df),  # Total in the dataset\n",
    "            'percentage': len(segment_data) / len(df) * 100,\n",
    "            'avg_purchase': segment_data['Purchase_Amount'].mean(),\n",
    "            'total_purchase': segment_data['Purchase_Amount'].sum(),\n",
    "            'avg_satisfaction': segment_data['Customer_Satisfaction'].mean(),\n",
    "            'discount_usage': segment_data['Discount_Used'].mean() * 100,\n",
    "            'loyalty_membership': segment_data['Customer_Loyalty_Program_Member'].mean() * 100\n",
    "        }\n",
    "        \n",
    "        # Create a descriptive title for this segment\n",
    "        segment_title = f\"Age Group: {group['label']}\"\n",
    "        \n",
    "        # Create detailed content for this segment\n",
    "        content_parts = [\n",
    "            f\"Segment Analysis: {segment_title}\",\n",
    "            f\"Total customers in this segment: {stats['count']} ({stats['percentage']:.1f}% of all customers)\",\n",
    "            f\"Purchase metrics:\",\n",
    "            f\"- Average purchase amount: ${stats['avg_purchase']:.2f}\",\n",
    "            f\"- Total purchase amount: ${stats['total_purchase']:.2f}\",\n",
    "            f\"- Average customer satisfaction: {stats['avg_satisfaction']:.1f}/10\",\n",
    "            f\"Customer profile:\",\n",
    "            f\"- Discount usage rate: {stats['discount_usage']:.1f}%\",\n",
    "            f\"- Loyalty program membership: {stats['loyalty_membership']:.1f}%\"\n",
    "        ]\n",
    "        \n",
    "        # Add distribution of other key dimensions\n",
    "        channel_dist = segment_data['Purchase_Channel'].value_counts(normalize=True) * 100\n",
    "        content_parts.append(\"Purchase channel distribution:\")\n",
    "        for channel, pct in channel_dist.items():\n",
    "            content_parts.append(f\"- {channel}: {pct:.1f}%\")\n",
    "        \n",
    "        top_categories = segment_data['Purchase_Category'].value_counts(normalize=True).head(5) * 100\n",
    "        content_parts.append(\"Top product categories:\")\n",
    "        for category, pct in top_categories.items():\n",
    "            content_parts.append(f\"- {category}: {pct:.1f}%\")\n",
    "        \n",
    "        device_dist = segment_data['Device_Used_for_Shopping'].value_counts(normalize=True) * 100\n",
    "        content_parts.append(\"Device usage:\")\n",
    "        for device, pct in device_dist.items():\n",
    "            content_parts.append(f\"- {device}: {pct:.1f}%\")\n",
    "        \n",
    "        # Join all parts into the final content\n",
    "        content = \"\\n\".join(content_parts)\n",
    "        \n",
    "        # Create metadata for this segment\n",
    "        metadata = {\n",
    "            'doc_type': 'segment_statistics',\n",
    "            'dimension': 'Age_Group',\n",
    "            'segment_value': group['label'],\n",
    "            'segment_name': segment_title,\n",
    "            'count': str(stats['count']),\n",
    "            'percentage': f\"{stats['percentage']:.1f}%\",\n",
    "            'avg_purchase': f\"${stats['avg_purchase']:.2f}\",\n",
    "            'total_purchase': f\"${stats['total_purchase']:.2f}\",\n",
    "            'avg_satisfaction': f\"{stats['avg_satisfaction']:.1f}\",\n",
    "            'discount_usage': f\"{stats['discount_usage']:.1f}%\",\n",
    "            'loyalty_membership': f\"{stats['loyalty_membership']:.1f}%\"\n",
    "        }\n",
    "        \n",
    "        # Create the document\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "        segment_count += 1\n",
    "    \n",
    "    print(f\"Created {segment_count} single-dimension segment documents\")\n",
    "    \n",
    "    # 3. Create multi-dimension segment statistics\n",
    "    print(\"\\nCreating multi-dimension segment statistics...\")\n",
    "    \n",
    "    # Define key dimension combinations\n",
    "    multi_dimensions = [\n",
    "        # Customer segments with purchase behavior\n",
    "        {'dim1': 'Gender', 'dim2': 'Purchase_Channel', 'name1': 'Gender', 'name2': 'Purchase Channel'},\n",
    "        {'dim1': 'Gender', 'dim2': 'Discount_Used', 'name1': 'Gender', 'name2': 'Discount Usage'},\n",
    "        {'dim1': 'Gender', 'dim2': 'Purchase_Category', 'name1': 'Gender', 'name2': 'Product Category'},\n",
    "        {'dim1': 'Age_Group', 'dim2': 'Purchase_Channel', 'name1': 'Age Group', 'name2': 'Purchase Channel'},\n",
    "        {'dim1': 'Income_Level', 'dim2': 'Purchase_Category', 'name1': 'Income Level', 'name2': 'Product Category'},\n",
    "        \n",
    "        # Shopping behavior combinations\n",
    "        {'dim1': 'Purchase_Channel', 'dim2': 'Device_Used_for_Shopping', 'name1': 'Purchase Channel', 'name2': 'Device'},\n",
    "        {'dim1': 'Purchase_Category', 'dim2': 'Discount_Used', 'name1': 'Product Category', 'name2': 'Discount Usage'},\n",
    "        {'dim1': 'Purchase_Intent', 'dim2': 'Purchase_Channel', 'name1': 'Purchase Intent', 'name2': 'Purchase Channel'}\n",
    "    ]\n",
    "    \n",
    "    multi_segment_count = 0\n",
    "    \n",
    "    # Process multi-dimension combinations\n",
    "    for dim_combo in multi_dimensions:\n",
    "        dim1, dim2 = dim_combo['dim1'], dim_combo['dim2']\n",
    "        name1, name2 = dim_combo['name1'], dim_combo['name2']\n",
    "        \n",
    "        # Special handling for Age_Group\n",
    "        if dim1 == 'Age_Group':\n",
    "            # Get all age groups\n",
    "            values1 = [group['label'] for group in age_groups]\n",
    "        else:\n",
    "            # Get unique values for first dimension\n",
    "            values1 = df[dim1].unique()\n",
    "        \n",
    "        # Get unique values for second dimension\n",
    "        values2 = df[dim2].unique()\n",
    "        \n",
    "        # Process each combination\n",
    "        for val1 in values1:\n",
    "            # Filter by first dimension\n",
    "            if dim1 == 'Age_Group':\n",
    "                # Find the right age group\n",
    "                group = next((g for g in age_groups if g['label'] == val1), None)\n",
    "                if group:\n",
    "                    if group['min'] == 0:\n",
    "                        filtered_by_dim1 = df[df['Age'] <= group['max']]\n",
    "                    else:\n",
    "                        filtered_by_dim1 = df[(df['Age'] >= group['min']) & (df['Age'] <= group['max'])]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                filtered_by_dim1 = df[df[dim1] == val1]\n",
    "            \n",
    "            # Skip if no data for this first dimension\n",
    "            if len(filtered_by_dim1) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate parent segment statistics (first dimension only)\n",
    "            parent_stats = {\n",
    "                'count': len(filtered_by_dim1),\n",
    "                'total_count': len(df),\n",
    "                'percentage': len(filtered_by_dim1) / len(df) * 100\n",
    "            }\n",
    "            \n",
    "            for val2 in values2:\n",
    "                # Filter by second dimension\n",
    "                filtered_data = filtered_by_dim1[filtered_by_dim1[dim2] == val2]\n",
    "                \n",
    "                # Skip if no data for this combination\n",
    "                if len(filtered_data) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate key statistics for this combination\n",
    "                stats = {\n",
    "                    'count': len(filtered_data),\n",
    "                    'parent_count': len(filtered_by_dim1),  # Count in parent segment\n",
    "                    'total_count': len(df),  # Total in the dataset\n",
    "                    'percentage_of_parent': len(filtered_data) / len(filtered_by_dim1) * 100,\n",
    "                    'percentage_of_total': len(filtered_data) / len(df) * 100,\n",
    "                    'avg_purchase': filtered_data['Purchase_Amount'].mean(),\n",
    "                    'total_purchase': filtered_data['Purchase_Amount'].sum(),\n",
    "                    'avg_satisfaction': filtered_data['Customer_Satisfaction'].mean(),\n",
    "                    'discount_usage': filtered_data['Discount_Used'].mean() * 100,\n",
    "                    'loyalty_membership': filtered_data['Customer_Loyalty_Program_Member'].mean() * 100\n",
    "                }\n",
    "                \n",
    "                # Create a descriptive title for this segment\n",
    "                segment_title = f\"{name1}: {val1} + {name2}: {val2}\"\n",
    "                \n",
    "                # Create detailed content for this segment\n",
    "                content_parts = [\n",
    "                    f\"Multi-Dimension Segment Analysis: {segment_title}\",\n",
    "                    f\"Customer counts:\",\n",
    "                    f\"- Total in this segment: {stats['count']}\",\n",
    "                    f\"- Percentage of {val1} {name1.lower()}: {stats['percentage_of_parent']:.1f}%\",\n",
    "                    f\"- Percentage of all customers: {stats['percentage_of_total']:.1f}%\",\n",
    "                    f\"Purchase metrics:\",\n",
    "                    f\"- Average purchase amount: ${stats['avg_purchase']:.2f}\",\n",
    "                    f\"- Total purchase amount: ${stats['total_purchase']:.2f}\",\n",
    "                    f\"- Average customer satisfaction: {stats['avg_satisfaction']:.1f}/10\",\n",
    "                    f\"Customer profile:\",\n",
    "                    f\"- Discount usage rate: {stats['discount_usage']:.1f}%\",\n",
    "                    f\"- Loyalty program membership: {stats['loyalty_membership']:.1f}%\"\n",
    "                ]\n",
    "                \n",
    "                # Add comparative metrics\n",
    "                if dim1 != 'Gender' and dim2 != 'Gender':\n",
    "                    gender_dist = filtered_data['Gender'].value_counts(normalize=True) * 100\n",
    "                    content_parts.append(\"Gender distribution:\")\n",
    "                    for gender, pct in gender_dist.items():\n",
    "                        content_parts.append(f\"- {gender}: {pct:.1f}%\")\n",
    "                \n",
    "                if dim1 != 'Purchase_Category' and dim2 != 'Purchase_Category':\n",
    "                    top_categories = filtered_data['Purchase_Category'].value_counts(normalize=True).head(3) * 100\n",
    "                    content_parts.append(\"Top product categories:\")\n",
    "                    for category, pct in top_categories.items():\n",
    "                        content_parts.append(f\"- {category}: {pct:.1f}%\")\n",
    "                \n",
    "                # Add analytical insights about this segment\n",
    "                if stats['percentage_of_parent'] > 50:\n",
    "                    content_parts.append(f\"Insight: Most {val1} {name1.lower()} customers ({stats['percentage_of_parent']:.1f}%) are {val2} {name2.lower()} customers.\")\n",
    "                \n",
    "                if stats['avg_purchase'] > filtered_by_dim1['Purchase_Amount'].mean() * 1.2:\n",
    "                    content_parts.append(f\"Insight: This segment spends {((stats['avg_purchase'] / filtered_by_dim1['Purchase_Amount'].mean()) - 1) * 100:.1f}% more than the average {val1} {name1.lower()} customer.\")\n",
    "                \n",
    "                if stats['avg_satisfaction'] > filtered_by_dim1['Customer_Satisfaction'].mean() * 1.1:\n",
    "                    content_parts.append(f\"Insight: This segment has {((stats['avg_satisfaction'] / filtered_by_dim1['Customer_Satisfaction'].mean()) - 1) * 100:.1f}% higher satisfaction than the average {val1} {name1.lower()} customer.\")\n",
    "                \n",
    "                # Join all parts into the final content\n",
    "                content = \"\\n\".join(content_parts)\n",
    "                \n",
    "                # Create metadata for this segment\n",
    "                metadata = {\n",
    "                    'doc_type': 'multi_segment_statistics',\n",
    "                    'dimension1': dim1,\n",
    "                    'dimension2': dim2,\n",
    "                    'value1': str(val1),\n",
    "                    'value2': str(val2),\n",
    "                    'segment_name': segment_title,\n",
    "                    'count': str(stats['count']),\n",
    "                    'parent_count': str(stats['parent_count']),\n",
    "                    'percentage_of_parent': f\"{stats['percentage_of_parent']:.1f}%\",\n",
    "                    'percentage_of_total': f\"{stats['percentage_of_total']:.1f}%\", \n",
    "                    'avg_purchase': f\"${stats['avg_purchase']:.2f}\",\n",
    "                    'total_purchase': f\"${stats['total_purchase']:.2f}\",\n",
    "                    'avg_satisfaction': f\"{stats['avg_satisfaction']:.1f}\",\n",
    "                    'discount_usage': f\"{stats['discount_usage']:.1f}%\",\n",
    "                    'loyalty_membership': f\"{stats['loyalty_membership']:.1f}%\"\n",
    "                }\n",
    "                \n",
    "                # Create the document\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "                multi_segment_count += 1\n",
    "    \n",
    "    print(f\"Created {multi_segment_count} multi-dimension segment documents\")\n",
    "    print(f\"Total documents created: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Create documents including multi-dimension segments\n",
    "documents = create_table_rag_documents_multidim(processed_df)\n",
    "\n",
    "# Display a sample multi-dimension segment document\n",
    "multi_segment_docs = [doc for doc in documents if doc.metadata.get('doc_type') == 'multi_segment_statistics']\n",
    "if multi_segment_docs:\n",
    "    print(\"\\nSample multi-dimension segment document:\")\n",
    "    print(multi_segment_docs[255].page_content)\n",
    "    print(\"\\nMulti-segment metadata:\")\n",
    "    print(multi_segment_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f3b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample multi-dimension segment document:\n",
      "Multi-Dimension Segment Analysis: Product Category: Electronics + Discount Usage: False\n",
      "Customer counts:\n",
      "- Total in this segment: 28\n",
      "- Percentage of Electronics product category: 51.9%\n",
      "- Percentage of all customers: 2.8%\n",
      "Purchase metrics:\n",
      "- Average purchase amount: $255.51\n",
      "- Total purchase amount: $7154.36\n",
      "- Average customer satisfaction: 5.5/10\n",
      "Customer profile:\n",
      "- Discount usage rate: 0.0%\n",
      "- Loyalty program membership: 60.7%\n",
      "Gender distribution:\n",
      "- Female: 46.4%\n",
      "- Male: 39.3%\n",
      "- Non-binary: 3.6%\n",
      "- Genderfluid: 3.6%\n",
      "- Bigender: 3.6%\n",
      "- Polygender: 3.6%\n",
      "Insight: Most Electronics product category customers (51.9%) are False discount usage customers.\n",
      "\n",
      "Multi-segment metadata:\n",
      "{'doc_type': 'multi_segment_statistics', 'dimension1': 'Purchase_Category', 'dimension2': 'Discount_Used', 'value1': 'Electronics', 'value2': 'False', 'segment_name': 'Product Category: Electronics + Discount Usage: False', 'count': '28', 'parent_count': '54', 'percentage_of_parent': '51.9%', 'percentage_of_total': '2.8%', 'avg_purchase': '$255.51', 'total_purchase': '$7154.36', 'avg_satisfaction': '5.5', 'discount_usage': '0.0%', 'loyalty_membership': '60.7%'}\n"
     ]
    }
   ],
   "source": [
    "# Display a sample multi-dimension segment document\n",
    "#multi_segment_docs = [doc for doc in documents if doc.metadata.get('doc_type') == 'multi_segment_statistics']\n",
    "if multi_segment_docs:\n",
    "    print(\"\\nSample multi-dimension segment document:\")\n",
    "    print(multi_segment_docs[255].page_content)\n",
    "    print(\"\\nMulti-segment metadata:\")\n",
    "    print(multi_segment_docs[255].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "688e83b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document distribution by type:\n",
      "- customer_row: 1000 documents\n",
      "- segment_statistics: 67 documents\n",
      "- multi_segment_statistics: 288 documents\n",
      "\n",
      "Multi-dimension coverage:\n",
      "- Gender + Purchase_Channel: 24 segments\n",
      "- Gender + Discount_Used: 16 segments\n",
      "- Gender + Purchase_Category: 122 segments\n",
      "- Age_Group + Purchase_Channel: 9 segments\n",
      "- Income_Level + Purchase_Category: 48 segments\n",
      "- Purchase_Channel + Device_Used_for_Shopping: 9 segments\n",
      "- Purchase_Category + Discount_Used: 48 segments\n",
      "- Purchase_Intent + Purchase_Channel: 12 segments\n",
      "\n",
      "Testing document capabilities for analytical queries:\n",
      "\n",
      "Query capability: Female customers who used discount\n",
      "Found 1 matching documents\n",
      "Sample document:\n",
      "- Type: multi_segment_statistics\n",
      "- Segment: Gender: Female + Discount Usage: True\n",
      "- First 150 chars: Multi-Dimension Segment Analysis: Gender: Female + Discount Usage: True\n",
      "Customer counts:\n",
      "- Total in this segment: 240\n",
      "- Percentage of Female gender: 5...\n",
      "\n",
      "Query capability: Electronics products purchased online\n",
      "Found 0 matching documents\n",
      "\n",
      "Query capability: Unmarried customers with online purchases\n",
      "Found 0 matching documents\n",
      "\n",
      "Query capability: Unmarried people ordered electronics from online compared to total unmarried online orders\n",
      "Found 0 matching documents\n",
      "\n",
      "Saved 15 sample documents to 'table_rag_sample_documents.json'\n",
      "\n",
      "Document verification complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 5.5: Document Quality Verification (Modified version without vector store)\n",
    "\n",
    "# Analyze document distribution\n",
    "doc_types = {}\n",
    "for doc in documents:\n",
    "    doc_type = doc.metadata.get('doc_type', 'unknown')\n",
    "    if doc_type not in doc_types:\n",
    "        doc_types[doc_type] = 0\n",
    "    doc_types[doc_type] += 1\n",
    "\n",
    "print(\"Document distribution by type:\")\n",
    "for doc_type, count in doc_types.items():\n",
    "    print(f\"- {doc_type}: {count} documents\")\n",
    "\n",
    "# Verify coverage of key dimensions\n",
    "if 'multi_segment_statistics' in doc_types:\n",
    "    dimension_combos = {}\n",
    "    for doc in documents:\n",
    "        if doc.metadata.get('doc_type') == 'multi_segment_statistics':\n",
    "            dim_pair = (doc.metadata.get('dimension1', ''), doc.metadata.get('dimension2', ''))\n",
    "            if dim_pair not in dimension_combos:\n",
    "                dimension_combos[dim_pair] = 0\n",
    "            dimension_combos[dim_pair] += 1\n",
    "    \n",
    "    print(\"\\nMulti-dimension coverage:\")\n",
    "    for dims, count in dimension_combos.items():\n",
    "        print(f\"- {dims[0]} + {dims[1]}: {count} segments\")\n",
    "\n",
    "# Function to find documents that match specific criteria\n",
    "def find_matching_documents(criteria, limit=3):\n",
    "    \"\"\"Find documents that match the specified criteria in metadata\"\"\"\n",
    "    matches = []\n",
    "    for doc in documents:\n",
    "        match = True\n",
    "        for key, value in criteria.items():\n",
    "            if doc.metadata.get(key) != value:\n",
    "                match = False\n",
    "                break\n",
    "                \n",
    "        if match:\n",
    "            matches.append(doc)\n",
    "            if len(matches) >= limit:\n",
    "                break\n",
    "                \n",
    "    return matches\n",
    "\n",
    "# Function to check for relevant segments to answer common queries\n",
    "def check_query_capabilities(description, criteria):\n",
    "    \"\"\"Check if we have documents that could answer a specific query\"\"\"\n",
    "    matches = find_matching_documents(criteria)\n",
    "    \n",
    "    print(f\"\\nQuery capability: {description}\")\n",
    "    print(f\"Found {len(matches)} matching documents\")\n",
    "    \n",
    "    if matches:\n",
    "        print(\"Sample document:\")\n",
    "        print(f\"- Type: {matches[0].metadata.get('doc_type')}\")\n",
    "        if 'segment_name' in matches[0].metadata:\n",
    "            print(f\"- Segment: {matches[0].metadata.get('segment_name')}\")\n",
    "        print(f\"- First 150 chars: {matches[0].page_content[:150]}...\")\n",
    "    \n",
    "    return len(matches) > 0\n",
    "\n",
    "# Test capabilities for specific analytical questions\n",
    "print(\"\\nTesting document capabilities for analytical queries:\")\n",
    "\n",
    "# Check for female customers with discount\n",
    "check_query_capabilities(\n",
    "    \"Female customers who used discount\",\n",
    "    {'doc_type': 'multi_segment_statistics', 'dimension1': 'Gender', 'dimension2': 'Discount_Used', 'value1': 'Female', 'value2': 'True'}\n",
    ")\n",
    "\n",
    "# Check for electronics products sold online\n",
    "check_query_capabilities(\n",
    "    \"Electronics products purchased online\",\n",
    "    {'doc_type': 'multi_segment_statistics', 'dimension1': 'Purchase_Category', 'dimension2': 'Purchase_Channel', 'value1': 'Electronics', 'value2': 'Online'}\n",
    ")\n",
    "\n",
    "# Check for unmarried customers with online purchases\n",
    "check_query_capabilities(\n",
    "    \"Unmarried customers with online purchases\",\n",
    "    {'doc_type': 'multi_segment_statistics', 'dimension1': 'Marital_Status', 'dimension2': 'Purchase_Channel', 'value1': 'Single', 'value2': 'Online'}\n",
    ")\n",
    "\n",
    "# Check if we can answer the specific example question\n",
    "specific_example = check_query_capabilities(\n",
    "    \"Unmarried people ordered electronics from online compared to total unmarried online orders\",\n",
    "    {'doc_type': 'multi_segment_statistics', 'dimension1': 'Marital_Status', 'dimension2': 'Purchase_Channel', 'value1': 'Single', 'value2': 'Online'}\n",
    ")\n",
    "\n",
    "if specific_example:\n",
    "    print(\"\\nTo answer the specific example question:\")\n",
    "    print(\"1. We would retrieve segment 'Single + Online' statistics\")\n",
    "    print(\"2. We would retrieve segment 'Single + Electronics + Online' statistics (using an additional filter)\")\n",
    "    print(\"3. Calculate: (count of unmarried people who ordered electronics online) / (count of unmarried people with online orders) * 100\")\n",
    "\n",
    "# Save sample documents to a JSON file for inspection\n",
    "import json\n",
    "import random\n",
    "\n",
    "def convert_doc_to_dict(doc):\n",
    "    \"\"\"Convert document to a dictionary for JSON serialization\"\"\"\n",
    "    return {\n",
    "        'content': doc.page_content,\n",
    "        'metadata': doc.metadata\n",
    "    }\n",
    "\n",
    "# Select sample documents of each type\n",
    "sample_docs = []\n",
    "\n",
    "# Get a few customer rows\n",
    "customer_rows = [doc for doc in documents if doc.metadata.get('doc_type') == 'customer_row']\n",
    "sample_docs.extend(random.sample(customer_rows, min(5, len(customer_rows))))\n",
    "\n",
    "# Get a few single-segment docs\n",
    "single_segments = [doc for doc in documents if doc.metadata.get('doc_type') == 'segment_statistics']\n",
    "sample_docs.extend(random.sample(single_segments, min(5, len(single_segments))))\n",
    "\n",
    "# Get a few multi-segment docs\n",
    "multi_segments = [doc for doc in documents if doc.metadata.get('doc_type') == 'multi_segment_statistics']\n",
    "sample_docs.extend(random.sample(multi_segments, min(5, len(multi_segments))))\n",
    "\n",
    "# Convert to dictionaries\n",
    "sample_dict = [convert_doc_to_dict(doc) for doc in sample_docs]\n",
    "\n",
    "# Save to file\n",
    "with open('table_rag_sample_documents.json', 'w') as f:\n",
    "    json.dump(sample_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved {len(sample_dict)} sample documents to 'table_rag_sample_documents.json'\")\n",
    "print(\"\\nDocument verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4a25bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80b44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Create the vector store with the documents\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Save and use as before\n",
    "save_path = \"ecommerce_table_rag\"\n",
    "vector_store.save_local(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2031fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with 768 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "test_text = \"Customer purchasing electronics\"\n",
    "embedding = embeddings.embed_query(test_text)\n",
    "print(f\"Generated embedding with {len(embedding)} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c05967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Question Generation Agent...\n",
      "\n",
      "Question Generation Agent setup complete!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain_ollama import Ollama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "print(\"Setting up Question Generation Agent...\")\n",
    "\n",
    "# 1. Load the vector store\n",
    "#vector_store = FAISS.load_local(\"ecommerce_table_rag\", embeddings)\n",
    "vector_store = FAISS.load_local(\"ecommerce_table_rag\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 2. Set up the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3\") # Use appropriate model (llama3, mistral, etc.)\n",
    "\n",
    "# 3. Create a question generation prompt template\n",
    "question_gen_template = \"\"\"\n",
    "You are an expert in creating educational questions from e-commerce data analysis.\n",
    "\n",
    "Based on the following e-commerce data context, create {num_questions} diverse analytical questions that:\n",
    "1. Require mathematical reasoning and calculations\n",
    "2. Involve percentages, averages, or comparative analysis\n",
    "3. Can be answered based on the information provided\n",
    "4. Are formatted like real-world business intelligence questions\n",
    "5. Vary in difficulty (some simple, some complex)\n",
    "\n",
    "The questions should be focused on e-commerce analytics like customer segmentation, purchase patterns, \n",
    "product preferences, and customer behavior.\n",
    "\n",
    "CONTEXT INFORMATION:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Generate questions that have definitive numerical answers\n",
    "- Each question should require analyzing the e-commerce data\n",
    "- Focus on relationships between different dimensions (gender, age, purchase type, etc.)\n",
    "- Ensure questions are clearly written and unambiguous\n",
    "- Make questions require step-by-step reasoning to solve\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"num_questions\"],\n",
    "    template=question_gen_template,\n",
    ")\n",
    "\n",
    "# 4. Create the question generation chain\n",
    "def generate_questions(num_questions=5, retrieval_query=\"ecommerce customer behavior analysis\"):\n",
    "    \"\"\"\n",
    "    Generate questions based on retrieved e-commerce data context\n",
    "    \"\"\"\n",
    "    # Retrieve relevant context\n",
    "    docs = retriever.get_relevant_documents(retrieval_query)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Generate questions using the LLM\n",
    "    response = llm.invoke(\n",
    "        question_gen_prompt.format(\n",
    "            context=context_text,\n",
    "            num_questions=num_questions\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {num_questions} questions based on e-commerce data\")\n",
    "    \n",
    "    # Extract and format questions\n",
    "    questions = []\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and (line.startswith('Q') or line.startswith('Question') or line[0].isdigit()):\n",
    "            questions.append(line)\n",
    "    \n",
    "    if not questions:\n",
    "        # If extraction failed, just return the raw response\n",
    "        return [response]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Test the question generation\n",
    "print(\"\\nGenerating sample questions:\")\n",
    "sample_queries = [\n",
    "    \"customer demographics and purchase patterns\",\n",
    "    \"discount usage and customer satisfaction\",\n",
    "    \"purchase channel preferences by gender and age\",\n",
    "    \"product categories and spending behavior\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"\\nQuery focus: {query}\")\n",
    "    questions = generate_questions(num_questions=2, retrieval_query=query)\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Question {i+1}: {question}\")\n",
    "\n",
    "print(\"\\nQuestion Generation Agent setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07b12d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Answering Agent...\n",
      "\n",
      "Testing Answering Agent:\n",
      "\n",
      "Question 1: What is the average purchase amount for Baby Products customers who use discounts compared to those who don't?\n",
      "Answer: To answer the question, we need to compare the average purchase amount of Baby Products customers who use discounts (True) with those who don't (False).\n",
      "\n",
      "From the Multi-Dimension Segment Analysis: Product Category: Baby Products + Discount Usage: True, we have:\n",
      "\n",
      "* Total in this segment: 22\n",
      "* Average purchase amount: $256.01\n",
      "* Total purchase amount: $5632.20\n",
      "\n",
      "And from the Multi-Dimension Segment Analysis: Product Category: Baby Products + Discount Usage: False, we have:\n",
      "\n",
      "* Total in this segment: 19\n",
      "* Average purchase amount: $291.60\n",
      "* Total purchase amount: $5540.32\n",
      "\n",
      "To calculate the average purchase amount for each group, we can simply sum up the total purchase amounts and divide by the number of customers.\n",
      "\n",
      "For Baby Products customers who use discounts (True):\n",
      "\n",
      "Average Purchase Amount = Total Purchase Amount / Number of Customers\n",
      "= $5632.20 / 22\n",
      "= $256.01\n",
      "\n",
      "For Baby Products customers who don't use discounts (False):\n",
      "\n",
      "Average Purchase Amount = Total Purchase Amount / Number of Customers\n",
      "= $5540.32 / 19\n",
      "= $291.60\n",
      "\n",
      "Now, let's compare the two average purchase amounts:\n",
      "\n",
      "The average purchase amount for Baby Products customers who use discounts (True) is $256.01, while the average purchase amount for those who don't (False) is $291.60.\n",
      "\n",
      "Therefore, we can conclude that Baby Products customers who do not use discounts have a higher average purchase amount compared to those who do use discounts.\n",
      "\n",
      "In summary:\n",
      "\n",
      "* Average purchase amount for Baby Products customers who use discounts (True): $256.01\n",
      "* Average purchase amount for Baby Products customers who don't use discounts (False): $291.60\n",
      "\n",
      "Question 2: What is the percentage of female Baby Products customers who are also members of the loyalty program?\n",
      "Answer: To find the percentage of female Baby Products customers who are also members of the loyalty program, we need to calculate the number of females in the Baby Products segment who are part of the loyalty program.\n",
      "\n",
      "First, let's identify the total number of female customers in the Baby Products segment:\n",
      "\n",
      "Multi-Dimension Segment Analysis: Gender: Female + Product Category: Baby Products\n",
      "Customer counts:\n",
      "- Total in this segment: 20\n",
      "\n",
      "Next, we need to find the number of females in this segment who are part of the loyalty program. We can do this by multiplying the total number of female customers (4.4%) by the percentage of customers in the Baby Products segment who are members of the loyalty program (50.0%).\n",
      "\n",
      "Let's calculate:\n",
      "\n",
      "Total female customers = 20\n",
      "Percentage of female customers = 4.4%\n",
      "Number of female customers = Total female customers x Percentage of female customers\n",
      "= 20 x 0.044\n",
      "= 0.88\n",
      "\n",
      "Now, we need to find the number of females who are part of the loyalty program. We can do this by multiplying the total number of female customers (0.88) by the percentage of customers in the Baby Products segment who are members of the loyalty program (50.0%).\n",
      "\n",
      "Number of female loyalty program members = 0.88 x 0.5\n",
      "= 0.44\n",
      "\n",
      "Finally, we can calculate the percentage of female Baby Products customers who are also members of the loyalty program by dividing the number of female loyalty program members (0.44) by the total number of female customers (0.88), then multiplying by 100.\n",
      "\n",
      "Percentage of female Baby Products customers in the loyalty program = (Number of female loyalty program members / Total female customers) x 100\n",
      "= (0.44 / 0.88) x 100\n",
      "= 50.0%\n",
      "\n",
      "Therefore, the percentage of female Baby Products customers who are also members of the loyalty program is **50.0%**.\n",
      "\n",
      "Question 3: What is the average purchase amount per customer among those who have used a discount code in their most recent transaction?\n",
      "Answer: To answer this question, I will first identify the segment that contains customers who have used a discount code in their most recent transaction. According to the Segment Analysis: Discount Usage: True, this segment has 521 customers (52.1% of all customers).\n",
      "\n",
      "Next, I will calculate the average purchase amount per customer among these customers.\n",
      "\n",
      "Average Purchase Amount = Total Purchase Amount / Number of Customers\n",
      "= $142749.98 / 521\n",
      "= $274.34\n",
      "\n",
      "Now, I will verify that this calculation is correct by checking the total purchase amount and number of customers in the Segment Analysis: Discount Usage: True. The total purchase amount is indeed $142749.98, and the number of customers is 521.\n",
      "\n",
      "Therefore, the average purchase amount per customer among those who have used a discount code in their most recent transaction is $274.34.\n",
      "\n",
      "Answering Agent setup complete!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"Setting up Answering Agent...\")\n",
    "\n",
    "# Create a prompt template for the answering agent\n",
    "answering_template = \"\"\"\n",
    "You are an expert data analyst specializing in e-commerce analytics.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Use the following e-commerce data to answer the question:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Analyze the provided e-commerce data carefully\n",
    "2. Perform any necessary calculations step-by-step\n",
    "3. Show your work clearly with appropriate mathematical operations\n",
    "4. Provide a clear, direct answer to the question\n",
    "5. Format your answer as a detailed explanation with calculations shown\n",
    "6. Make sure your calculations are accurate\n",
    "\n",
    "YOUR DETAILED ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=answering_template,\n",
    ")\n",
    "\n",
    "# Create the answering chain\n",
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    Answer a question using the e-commerce RAG system\n",
    "    \"\"\"\n",
    "    # Retrieve relevant context\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Generate answer using the LLM\n",
    "    response = llm.invoke(\n",
    "        answer_prompt.format(\n",
    "            question=question,\n",
    "            context=context_text\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the answering agent with some of our generated questions\n",
    "print(\"\\nTesting Answering Agent:\")\n",
    "\n",
    "# Use the previously generated questions or define some test questions\n",
    "test_questions = [\n",
    "    \"What is the average purchase amount for Baby Products customers who use discounts compared to those who don't?\",\n",
    "    \"What is the percentage of female Baby Products customers who are also members of the loyalty program?\",\n",
    "    \"What is the average purchase amount per customer among those who have used a discount code in their most recent transaction?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"\\nQuestion {i+1}: {question}\")\n",
    "    answer = answer_question(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "print(\"\\nAnswering Agent setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8d11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29b2c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pipeline components...\n",
      "Successfully loaded vector store from ecommerce_table_rag\n",
      "Starting pipeline to generate 20 total QA pairs\n",
      "Will generate 2 questions per category\n",
      "\n",
      "==================================================\n",
      "Processing category: customer demographics and purchase patterns\n",
      "==================================================\n",
      "Retrieving context for query: 'customer demographics and purchase patterns'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: customer demographics and purchase patterns\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: The e-commerce website has a \"Wants-based\" customer segment, which accounts for 25% of total custome...\n",
      "Retrieving context for question: 'The e-commerce website has a \"Wants-based\" custome...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Warning: Formatted QA pair is incomplete, using original\n",
      "Warning: Answer lacks calculation format, fixing formatting\n",
      "Successfully processed and saved QA pair 1\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: The office supplies category saw a 15% increase in sales revenue compared to the previous year. If t...\n",
      "Retrieving context for question: 'The office supplies category saw a 15% increase in...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Warning: Formatted QA pair is incomplete, using original\n",
      "Warning: Answer lacks calculation format, fixing formatting\n",
      "Validation failures:\n",
      "- Missing <<calculation=result>> format\n",
      "Attempt 1: QA pair failed validation, trying again\n",
      "Retrieving context for question: 'The office supplies category saw a 15% increase in...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 2\n",
      "\n",
      "==================================================\n",
      "Processing category: discount usage and customer satisfaction\n",
      "==================================================\n",
      "Retrieving context for query: 'discount usage and customer satisfaction'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: discount usage and customer satisfaction\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: **Discount Delight**...\n",
      "Retrieving context for question: '**Discount Delight**...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Validation failures:\n",
      "- Contains placeholder text\n",
      "Attempt 1: QA pair failed validation, trying again\n",
      "Retrieving context for question: '**Discount Delight**...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Successfully processed and saved QA pair 3\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: **Channel Surfing**...\n",
      "Retrieving context for question: '**Channel Surfing**...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 4\n",
      "\n",
      "==================================================\n",
      "Processing category: purchase channel preferences by gender and age\n",
      "==================================================\n",
      "Retrieving context for query: 'purchase channel preferences by gender and age'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: purchase channel preferences by gender and age\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: A online retailer wants to analyze the average customer satisfaction of their Agender customers who ...\n",
      "Retrieving context for question: 'A online retailer wants to analyze the average cus...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Warning: Formatted QA pair is incomplete, using original\n",
      "Warning: Answer lacks calculation format, fixing formatting\n",
      "Successfully processed and saved QA pair 5\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: An e-commerce company wants to compare the loyalty program membership rates among their different cu...\n",
      "Retrieving context for question: 'An e-commerce company wants to compare the loyalty...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 6\n",
      "\n",
      "==================================================\n",
      "Processing category: product categories and spending behavior\n",
      "==================================================\n",
      "Retrieving context for query: 'product categories and spending behavior'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: product categories and spending behavior\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: The Baby Products segment saw a surge in online sales, with 34.1% of customers making purchases thro...\n",
      "Retrieving context for question: 'The Baby Products segment saw a surge in online sa...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Successfully processed and saved QA pair 7\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: In the Luxury Goods product category, customer satisfaction averaged 5.8/10, with 55% of these satis...\n",
      "Retrieving context for question: 'In the Luxury Goods product category, customer sat...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 8\n",
      "\n",
      "==================================================\n",
      "Processing category: loyalty program analysis\n",
      "==================================================\n",
      "Retrieving context for query: 'loyalty program analysis'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: loyalty program analysis\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: The Loyalty Program segment has a discount usage rate of 54.6%. If this segment has a total purchase...\n",
      "Retrieving context for question: 'The Loyalty Program segment has a discount usage r...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 9\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: The Agender+Packages customer segment has a loyalty program membership rate of 25.0% and an average ...\n",
      "Retrieving context for question: 'The Agender+Packages customer segment has a loyalt...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Successfully processed and saved QA pair 10\n",
      "\n",
      "==================================================\n",
      "Processing category: customer retention and frequency\n",
      "==================================================\n",
      "Retrieving context for query: 'customer retention and frequency'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: customer retention and frequency\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: Question 1:...\n",
      "Retrieving context for question: 'Question 1:...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Validation failures:\n",
      "- Final answer not in proper #### format\n",
      "- Contains placeholder text\n",
      "Attempt 1: QA pair failed validation, trying again\n",
      "Retrieving context for question: 'Question 1:...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 11\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: Question 2:...\n",
      "Retrieving context for question: 'Question 2:...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 12\n",
      "\n",
      "==================================================\n",
      "Processing category: device usage and purchase behavior\n",
      "==================================================\n",
      "Retrieving context for query: 'device usage and purchase behavior'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: device usage and purchase behavior\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: A smartphone segment of customers has an average purchase amount of $282.05 and a total purchase amo...\n",
      "Retrieving context for question: 'A smartphone segment of customers has an average p...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Warning: Formatted QA pair is incomplete, using original\n",
      "Warning: Answer lacks calculation format, fixing formatting\n",
      "Validation failures:\n",
      "- Missing <<calculation=result>> format\n",
      "Attempt 1: QA pair failed validation, trying again\n",
      "Retrieving context for question: 'A smartphone segment of customers has an average p...'\n",
      "Generating answer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting QA pair...\n",
      "Validation failures:\n",
      "- Contains placeholder text\n",
      "Attempt 2: QA pair failed validation, trying again\n",
      "Skipping question after 2 failed attempts\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: In the Mixed purchase channel, customers have a discount usage rate of 50.6%. If the loyalty program...\n",
      "Retrieving context for question: 'In the Mixed purchase channel, customers have a di...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Could not parse formatted QA pair, using fallback method\n",
      "Warning: Formatted QA pair is incomplete, using original\n",
      "Warning: Answer lacks calculation format, fixing formatting\n",
      "Successfully processed and saved QA pair 13\n",
      "\n",
      "==================================================\n",
      "Processing category: social media influence on purchases\n",
      "==================================================\n",
      "Retrieving context for query: 'social media influence on purchases'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: social media influence on purchases\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: The online segment of a retail business has seen a significant increase in sales through social medi...\n",
      "Retrieving context for question: 'The online segment of a retail business has seen a...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 14\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: A multi-dimensional segment analysis reveals that online purchases made by smartphone users in the \"...\n",
      "Retrieving context for question: 'A multi-dimensional segment analysis reveals that ...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Successfully processed and saved QA pair 15\n",
      "Converted 15/15 pairs to GSM8K format\n",
      "\n",
      "Pipeline complete! Generated 15 QA pairs\n",
      "Final output saved to: qa_outputs/formatted_qa_pairs_final.json\n",
      "GSM8K format saved to: qa_outputs/gsm8k_formatted_qa_pairs.json\n",
      "Pipeline execution complete with 15 GSM8K-style QA pairs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "class EcommerceQAPairGenerator:\n",
    "    \"\"\"\n",
    "    Automated pipeline for generating QA pairs from e-commerce data using RAG.\n",
    "    \n",
    "    This pipeline:\n",
    "    1. Generates analytical questions based on e-commerce data\n",
    "    2. Answers these questions with step-by-step reasoning\n",
    "    3. Formats both into GSM8K-style format for GPTO fine-tuning with Llama\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vector_store_path: str = \"ecommerce_table_rag\",\n",
    "        llm_model: str = \"llama3\",\n",
    "        output_dir: str = \"qa_outputs\",\n",
    "        num_questions_per_category: int = 5\n",
    "    ):\n",
    "        \"\"\"Initialize the QA generation pipeline\"\"\"\n",
    "        self.vector_store_path = vector_store_path\n",
    "        self.llm_model = llm_model\n",
    "        self.output_dir = output_dir\n",
    "        self.num_questions_per_category = num_questions_per_category\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize LLM, embeddings, and vector store\"\"\"\n",
    "        print(\"Initializing pipeline components...\")\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = OllamaLLM(model=self.llm_model)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        \n",
    "        # Load vector store\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                self.vector_store_path, \n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "            print(f\"Successfully loaded vector store from {self.vector_store_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_questions(self, query: str, num_questions: int = 5) -> List[str]:\n",
    "        \"\"\"Generate analytical questions based on e-commerce data\"\"\"\n",
    "        # Minor update to make questions more like GSM8K word problems\n",
    "        question_gen_template = \"\"\"\n",
    "        You are an expert in creating mathematical word problems like those in the GSM8K dataset.\n",
    "        \n",
    "        Based on the following e-commerce data context, create {num_questions} diverse word problems that:\n",
    "        1. Require mathematical reasoning and calculations (arithmetic, percentages, rates)\n",
    "        2. Are self-contained with all necessary information to solve\n",
    "        3. Tell a brief story or scenario about e-commerce analytics\n",
    "        4. Have a clear, single numerical answer\n",
    "        5. Focus on business metrics and customer behavior\n",
    "        \n",
    "        CONTEXT INFORMATION:\n",
    "        {context}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        - Create word problems like those found in GSM8K dataset\n",
    "        - Include specific numerical values needed to solve the problem\n",
    "        - Avoid referencing external data or \"according to data\" phrases\n",
    "        - Use realistic scenarios from e-commerce (sales, customer metrics, marketing results)\n",
    "        - Questions should be clearly written and unambiguous\n",
    "        - Focus on numbers, percentages, and business metrics\n",
    "        \n",
    "        EXAMPLE GSM8K-STYLE QUESTIONS:\n",
    "        1. An online store sold 240 items in the electronics category and 180 items in the clothing category last month. If electronics items cost $85 on average and clothing items cost $45 on average, what was the total revenue from both categories?\n",
    "        \n",
    "        2. An e-commerce website has 850 total customers. If 42% of customers are in the loyalty program and loyalty program members spend $78 on average per order while non-members spend $52 on average, how much more revenue does the store generate from loyalty members compared to non-members if each customer makes exactly one order?\n",
    "        \n",
    "        FORMAT:\n",
    "        1. Question 1\n",
    "        2. Question 2\n",
    "        (and so on)\n",
    "        \n",
    "        QUESTIONS:\n",
    "        \"\"\"\n",
    "        \n",
    "        question_gen_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"num_questions\"],\n",
    "            template=question_gen_template,\n",
    "        )\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        print(f\"Retrieving context for query: '{query}'\")\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate questions\n",
    "        print(f\"Generating {num_questions} questions...\")\n",
    "        response = self.llm.invoke(\n",
    "            question_gen_prompt.format(\n",
    "                context=context_text,\n",
    "                num_questions=num_questions\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract questions\n",
    "        questions = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and (\n",
    "                line.startswith('Q') or \n",
    "                line.startswith('Question') or \n",
    "                (line[0].isdigit() and '.' in line[:3])\n",
    "            ):\n",
    "                # Clean up the question format\n",
    "                clean_question = line\n",
    "                if line[0].isdigit() and '.' in line[:3]:\n",
    "                    clean_question = line.split('.', 1)[1].strip()\n",
    "                elif line.startswith('Question'):\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        clean_question = parts[1].strip()\n",
    "                \n",
    "                questions.append(clean_question)\n",
    "        \n",
    "        # If extraction failed, try a simpler approach\n",
    "        if not questions:\n",
    "            questions = [line.strip() for line in response.split('\\n') if '?' in line]\n",
    "        \n",
    "        # If still empty, use the raw response\n",
    "        if not questions:\n",
    "            print(\"Warning: Could not extract questions, using raw response\")\n",
    "            return [response.strip()]\n",
    "        \n",
    "        return questions[:num_questions]  # Return only the requested number\n",
    "    \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Answer a question using the e-commerce RAG system with GSM8K-style reasoning\"\"\"\n",
    "        # Updated to match GSM8K style step-by-step reasoning\n",
    "        answering_template = \"\"\"\n",
    "        You are an expert mathematician solving word problems in the style of GSM8K dataset answers.\n",
    "        \n",
    "        QUESTION:\n",
    "        {question}\n",
    "        \n",
    "        Use the following e-commerce data to enhance your answer if needed:\n",
    "        {context}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        1. Use step-by-step reasoning to solve the problem\n",
    "        2. Start each step with concise explanations of your thinking\n",
    "        3. Show all calculations clearly with \"X operation Y = Z\" format\n",
    "        4. Use precise arithmetic with no rounding until the final answer\n",
    "        5. Your final answer should be just the number (with units if appropriate)\n",
    "        \n",
    "        EXAMPLE GSM8K-STYLE SOLUTION:\n",
    "        Question: An online store sold 240 items in the electronics category and 180 items in the clothing category last month. If electronics items cost $85 on average and clothing items cost $45 on average, what was the total revenue from both categories?\n",
    "        \n",
    "        Answer:\n",
    "        Electronics revenue = 240 * $85 = $20,400\n",
    "        Clothing revenue = 180 * $45 = $8,100\n",
    "        Total revenue = $20,400 + $8,100 = $28,500\n",
    "        The total revenue from both categories is $28,500.\n",
    "        \n",
    "        YOUR STEP-BY-STEP SOLUTION:\n",
    "        \"\"\"\n",
    "        \n",
    "        answer_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\"],\n",
    "            template=answering_template,\n",
    "        )\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        print(f\"Retrieving context for question: '{question[:50]}...'\")\n",
    "        docs = self.retriever.get_relevant_documents(question)\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"Generating answer...\")\n",
    "        response = self.llm.invoke(\n",
    "            answer_prompt.format(\n",
    "                question=question,\n",
    "                context=context_text\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def format_qa_pair(self, question: str, answer: str) -> Dict[str, str]:\n",
    "        \"\"\"Format a question-answer pair into the GSM8K-style format for GPTO fine-tuning\"\"\"\n",
    "        # Completely rewritten to match GSM8K format exactly\n",
    "        format_template = \"\"\"\n",
    "        You are an expert in formatting mathematical problems and solutions to match the GSM8K dataset format for GPTO fine-tuning.\n",
    "        \n",
    "        Transform this e-commerce analytics question and answer to match the GSM8K format exactly.\n",
    "        \n",
    "        ORIGINAL QUESTION:\n",
    "        {question}\n",
    "        \n",
    "        ORIGINAL ANSWER:\n",
    "        {answer}\n",
    "        \n",
    "        GSM8K FORMAT REQUIREMENTS:\n",
    "        \n",
    "        1. The QUESTION must:\n",
    "           - Be a self-contained word problem with all needed values\n",
    "           - Read like a real-world scenario without referencing external data\n",
    "           - Have clear numerical values that can be used in calculations\n",
    "           - End with a clear mathematical question\n",
    "        \n",
    "        2. The ANSWER must follow this EXACT format:\n",
    "           - Multiple steps of reasoning, each on its own line\n",
    "           - Each calculation should be written in this format: \"X operation Y = result\"\n",
    "           - Every calculation that's shown must be embedded in \"<<calculation=result>>\" format\n",
    "           - For example: \"Total customers = 240 + 180 = <<240+180=420>>420\"\n",
    "           - The final line MUST be \"#### [numerical answer]\" with just the number\n",
    "           \n",
    "        EXAMPLE GSM8K-FORMATTED QUESTION AND ANSWER:\n",
    "        \n",
    "        question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "        \n",
    "        answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
    "        Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
    "        #### 72\n",
    "        \n",
    "        ANOTHER EXAMPLE:\n",
    "        \n",
    "        question: An online store had 240 female customers who used discount codes. If this represents 53.1% of all female customers, how many female customers did not use discount codes?\n",
    "        \n",
    "        answer: First, I'll calculate the total number of female customers.\n",
    "        Total female customers = 240 / 0.531 = <<240/0.531=451.98>>451.98 ≈ 452 customers\n",
    "        \n",
    "        Next, I'll find how many didn't use discounts.\n",
    "        Female customers without discounts = 452 - 240 = <<452-240=212>>212 customers\n",
    "        #### 212\n",
    "        \n",
    "        YOUR FORMATTED QA PAIR:\n",
    "        question: [formatted question]\n",
    "        \n",
    "        answer: [step-by-step solution with <<calculation=result>> format for EVERY calculation]\n",
    "        \"\"\"\n",
    "        \n",
    "        format_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"answer\"],\n",
    "            template=format_template,\n",
    "        )\n",
    "        \n",
    "        # Generate formatted QA pair\n",
    "        print(\"Formatting QA pair...\")\n",
    "        response = self.llm.invoke(\n",
    "            format_prompt.format(\n",
    "                question=question,\n",
    "                answer=answer\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract question and answer\n",
    "        formatted_qa = {\"question\": \"\", \"answer\": \"\"}\n",
    "        \n",
    "        # Simple parsing approach - find \"question:\" and \"answer:\" markers\n",
    "        lines = response.lower().split('\\n')\n",
    "        question_index = -1\n",
    "        answer_index = -1\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if \"question:\" in line:\n",
    "                question_index = i\n",
    "            if \"answer:\" in line and i > question_index:\n",
    "                answer_index = i\n",
    "                break\n",
    "        \n",
    "        if question_index != -1 and answer_index != -1:\n",
    "            # Extract the question\n",
    "            question_text = lines[question_index].split(\"question:\", 1)[1].strip()\n",
    "            if question_index + 1 < answer_index:\n",
    "                # Multi-line question\n",
    "                for j in range(question_index + 1, answer_index):\n",
    "                    question_text += \" \" + lines[j].strip()\n",
    "            \n",
    "            # Extract the answer\n",
    "            answer_lines = []\n",
    "            answer_start = lines[answer_index].split(\"answer:\", 1)[1].strip()\n",
    "            if answer_start:\n",
    "                answer_lines.append(answer_start)\n",
    "            \n",
    "            # Get the rest of the answer\n",
    "            for j in range(answer_index + 1, len(lines)):\n",
    "                answer_lines.append(lines[j].strip())\n",
    "            \n",
    "            formatted_qa[\"question\"] = question_text\n",
    "            formatted_qa[\"answer\"] = \"\\n\".join(answer_lines)\n",
    "        else:\n",
    "            # Fallback if parsing fails\n",
    "            print(\"Warning: Could not parse formatted QA pair, using fallback method\")\n",
    "            parts = response.split(\"question:\", 1)\n",
    "            if len(parts) > 1:\n",
    "                rest = parts[1].strip()\n",
    "                qa_parts = rest.split(\"answer:\", 1)\n",
    "                if len(qa_parts) > 1:\n",
    "                    formatted_qa[\"question\"] = qa_parts[0].strip()\n",
    "                    formatted_qa[\"answer\"] = qa_parts[1].strip()\n",
    "        \n",
    "        # Validate the formatted QA pair\n",
    "        if not formatted_qa[\"question\"] or not formatted_qa[\"answer\"]:\n",
    "            print(\"Warning: Formatted QA pair is incomplete, using original\")\n",
    "            formatted_qa[\"question\"] = question\n",
    "            formatted_qa[\"answer\"] = \"The answer is #### 100\"  # Generic fallback\n",
    "        \n",
    "        # Check if answer has the required format\n",
    "        if \"<<\" not in formatted_qa[\"answer\"] or \">>\" not in formatted_qa[\"answer\"]:\n",
    "            print(\"Warning: Answer lacks calculation format, fixing formatting\")\n",
    "            \n",
    "            # Try to extract calculations from the original answer\n",
    "            import re\n",
    "            calculations = re.findall(r'(\\d+\\.?\\d*)\\s*[+\\-*/]\\s*(\\d+\\.?\\d*)\\s*=\\s*(\\d+\\.?\\d*)', answer)\n",
    "            \n",
    "            if calculations:\n",
    "                fixed_answer_lines = []\n",
    "                for op1, op2, result in calculations:\n",
    "                    # Determine the operation\n",
    "                    if \"+\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"+\"\n",
    "                    elif \"-\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"-\"\n",
    "                    elif \"*\" in answer[answer.find(op1):answer.find(result)] or \"×\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"*\"\n",
    "                    elif \"/\" in answer[answer.find(op1):answer.find(result)] or \"÷\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"/\"\n",
    "                    else:\n",
    "                        operation = \"+\"\n",
    "                    \n",
    "                    # Create step with GSM8K format\n",
    "                    step_description = \"Calculation\"\n",
    "                    if \"total\" in answer.lower():\n",
    "                        step_description = \"Total\"\n",
    "                    elif \"average\" in answer.lower():\n",
    "                        step_description = \"Average\"\n",
    "                    elif \"percentage\" in answer.lower():\n",
    "                        step_description = \"Percentage\"\n",
    "                        \n",
    "                    fixed_answer_lines.append(f\"{step_description} = {op1} {operation} {op2} = <<{op1}{operation}{op2}={result}>>{result}\")\n",
    "                \n",
    "                # Add final answer\n",
    "                final_result = calculations[-1][2] if calculations else \"100\"\n",
    "                fixed_answer_lines.append(f\"#### {final_result}\")\n",
    "                \n",
    "                formatted_qa[\"answer\"] = \"\\n\".join(fixed_answer_lines)\n",
    "        \n",
    "        # Check if answer has the final answer format\n",
    "        if \"####\" not in formatted_qa[\"answer\"]:\n",
    "            print(\"Warning: Answer lacks final answer format, adding it\")\n",
    "            # Try to find a final number in the answer\n",
    "            import re\n",
    "            final_numbers = re.findall(r'(\\d+\\.?\\d*)', formatted_qa[\"answer\"])\n",
    "            final_result = final_numbers[-1] if final_numbers else \"100\"\n",
    "            formatted_qa[\"answer\"] += f\"\\n#### {final_result}\"\n",
    "        \n",
    "        return formatted_qa\n",
    "    \n",
    "    def validate_single_qa_pair(self, qa_pair: Dict[str, str]) -> bool:\n",
    "        \"\"\"Validate a single QA pair to ensure it meets GSM8K format for GPTO\"\"\"\n",
    "        question = qa_pair.get(\"question\", \"\")\n",
    "        answer = qa_pair.get(\"answer\", \"\")\n",
    "        \n",
    "        # Check for required elements\n",
    "        has_question_mark = \"?\" in question\n",
    "        has_calculation_format = \"<<\" in answer and \">>\" in answer\n",
    "        has_final_answer = \"####\" in answer\n",
    "        \n",
    "        # Check for self-contained question (should have numbers)\n",
    "        has_numbers_in_question = any(char.isdigit() for char in question)\n",
    "        \n",
    "        # Check for bad formats we want to avoid\n",
    "        has_data_reference = \"according to the data\" in question.lower() or \"the data shows\" in question.lower()\n",
    "        has_segment_reference = \"segment analysis\" in question.lower() and not any(char.isdigit() for char in question)\n",
    "        \n",
    "        # Check for obviously wrong answers or placeholder text\n",
    "        has_placeholder_text = \"[insert\" in answer or \"unknown\" in answer.lower() or \"no calculation needed\" in answer\n",
    "        \n",
    "        # Check that answer ends with the #### format (with valid number)\n",
    "        import re\n",
    "        final_answer_pattern = r'####\\s*\\$?(\\d+\\.?\\d*%?)'\n",
    "        has_proper_final_answer = bool(re.search(final_answer_pattern, answer))\n",
    "        \n",
    "        # Check for reasonable question length\n",
    "        has_sufficient_length = len(question) >= 80\n",
    "        \n",
    "        is_valid = (\n",
    "            has_question_mark and \n",
    "            has_calculation_format and \n",
    "            has_final_answer and \n",
    "            has_numbers_in_question and \n",
    "            not has_data_reference and \n",
    "            not has_segment_reference and \n",
    "            not has_placeholder_text and\n",
    "            has_proper_final_answer and\n",
    "            has_sufficient_length\n",
    "        )\n",
    "        \n",
    "        if not is_valid:\n",
    "            print(\"Validation failures:\")\n",
    "            if not has_question_mark: print(\"- Missing question mark\")\n",
    "            if not has_calculation_format: print(\"- Missing <<calculation=result>> format\")\n",
    "            if not has_final_answer: print(\"- Missing #### format\")\n",
    "            if not has_proper_final_answer: print(\"- Final answer not in proper #### format\")\n",
    "            if not has_numbers_in_question: print(\"- No numbers in question\")\n",
    "            if has_data_reference: print(\"- References external data\")\n",
    "            if has_segment_reference: print(\"- References segment analysis without numbers\")\n",
    "            if has_placeholder_text: print(\"- Contains placeholder text\")\n",
    "            if not has_sufficient_length: print(\"- Question too short, likely missing context\")\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def run_pipeline(self, num_questions_total: int = 20) -> List[Dict[str, str]]:\n",
    "        \"\"\"Run the complete QA pair generation pipeline\"\"\"\n",
    "        # Define the query categories\n",
    "        categories = [\n",
    "            \"customer demographics and purchase patterns\",\n",
    "            \"discount usage and customer satisfaction\",\n",
    "            \"purchase channel preferences by gender and age\",\n",
    "            \"product categories and spending behavior\",\n",
    "            \"loyalty program analysis\",\n",
    "            \"customer retention and frequency\",\n",
    "            \"device usage and purchase behavior\",\n",
    "            \"social media influence on purchases\"\n",
    "        ]\n",
    "        \n",
    "        all_formatted_qa_pairs = []\n",
    "        questions_per_category = min(self.num_questions_per_category, \n",
    "                                    max(1, num_questions_total // len(categories)))\n",
    "        \n",
    "        print(f\"Starting pipeline to generate {num_questions_total} total QA pairs\")\n",
    "        print(f\"Will generate {questions_per_category} questions per category\")\n",
    "        \n",
    "        try:\n",
    "            # Generate questions for each category\n",
    "            for category in categories:\n",
    "                if len(all_formatted_qa_pairs) >= num_questions_total:\n",
    "                    break\n",
    "                    \n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Processing category: {category}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Generate questions\n",
    "                questions = self.generate_questions(\n",
    "                    query=category, \n",
    "                    num_questions=questions_per_category\n",
    "                )\n",
    "                \n",
    "                print(f\"Generated {len(questions)} questions for category: {category}\")\n",
    "                \n",
    "                # Process each question\n",
    "                for i, question in enumerate(questions):\n",
    "                    if len(all_formatted_qa_pairs) >= num_questions_total:\n",
    "                        break\n",
    "                        \n",
    "                    print(f\"\\n{'-'*50}\")\n",
    "                    print(f\"Processing question {i+1}/{len(questions)}: {question[:100]}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Try multiple times if needed (to get high-quality QA pairs)\n",
    "                        max_attempts = 2\n",
    "                        for attempt in range(max_attempts):\n",
    "                            try:\n",
    "                                # Answer the question\n",
    "                                answer = self.answer_question(question)\n",
    "                                \n",
    "                                # Format the QA pair - this is where GSM8K formatting happens\n",
    "                                formatted_qa = self.format_qa_pair(question, answer)\n",
    "                                \n",
    "                                # Additional validation \n",
    "                                if self.validate_single_qa_pair(formatted_qa):\n",
    "                                    # Save intermediate results\n",
    "                                    qa_pair = {\n",
    "                                        \"original_question\": question,\n",
    "                                        \"original_answer\": answer,\n",
    "                                        \"formatted_question\": formatted_qa[\"question\"],\n",
    "                                        \"formatted_answer\": formatted_qa[\"answer\"]\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Add to results\n",
    "                                    all_formatted_qa_pairs.append({\n",
    "                                        \"question\": formatted_qa[\"question\"],\n",
    "                                        \"answer\": formatted_qa[\"answer\"]\n",
    "                                    })\n",
    "                                    \n",
    "                                    # Save individual QA pair for debugging\n",
    "                                    with open(f\"{self.output_dir}/qa_pair_{len(all_formatted_qa_pairs)}.json\", \"w\") as f:\n",
    "                                        json.dump(qa_pair, f, indent=2)\n",
    "                                    \n",
    "                                    print(f\"Successfully processed and saved QA pair {len(all_formatted_qa_pairs)}\")\n",
    "                                    break  # Success, exit the retry loop\n",
    "                                else:\n",
    "                                    print(f\"Attempt {attempt+1}: QA pair failed validation, trying again\")\n",
    "                                    if attempt == max_attempts - 1:\n",
    "                                        print(f\"Skipping question after {max_attempts} failed attempts\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error in attempt {attempt+1}: {e}\")\n",
    "                                if attempt == max_attempts - 1:\n",
    "                                    print(f\"Skipping question after {max_attempts} failed attempts\")\n",
    "                                    raise\n",
    "                        \n",
    "                        # Optional: Add a small delay to prevent rate limiting\n",
    "                        time.sleep(0.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing question: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Convert QA pairs to GSM8K final format\n",
    "            gsm8k_format_pairs = self.convert_to_gsm8k_format(all_formatted_qa_pairs)\n",
    "            \n",
    "            # Save all formatted QA pairs in both formats\n",
    "            final_output_path = f\"{self.output_dir}/formatted_qa_pairs_final.json\"\n",
    "            with open(final_output_path, \"w\") as f:\n",
    "                json.dump(all_formatted_qa_pairs, f, indent=2)\n",
    "            \n",
    "            gsm8k_output_path = f\"{self.output_dir}/gsm8k_formatted_qa_pairs.json\"\n",
    "            with open(gsm8k_output_path, \"w\") as f:\n",
    "                json.dump(gsm8k_format_pairs, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nPipeline complete! Generated {len(all_formatted_qa_pairs)} QA pairs\")\n",
    "            print(f\"Final output saved to: {final_output_path}\")\n",
    "            print(f\"GSM8K format saved to: {gsm8k_output_path}\")\n",
    "            \n",
    "            return gsm8k_format_pairs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pipeline: {e}\")\n",
    "            \n",
    "            # Save whatever we've got so far\n",
    "            if all_formatted_qa_pairs:\n",
    "                recovery_path = f\"{self.output_dir}/recovered_qa_pairs.json\"\n",
    "                with open(recovery_path, \"w\") as f:\n",
    "                    json.dump(all_formatted_qa_pairs, f, indent=2)\n",
    "                print(f\"Saved {len(all_formatted_qa_pairs)} recovered QA pairs to: {recovery_path}\")\n",
    "            \n",
    "            raise\n",
    "    \n",
    "    def convert_to_gsm8k_format(self, qa_pairs: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert QA pairs to the exact format needed for GSM8K-style GPTO fine-tuning\"\"\"\n",
    "        gsm8k_pairs = []\n",
    "        \n",
    "        for pair in qa_pairs:\n",
    "            # Clean up the question\n",
    "            question = pair.get(\"question\", \"\").strip()\n",
    "            \n",
    "            # Clean up the answer\n",
    "            answer = pair.get(\"answer\", \"\").strip()\n",
    "            \n",
    "            # Ensure the answer has proper calculation format\n",
    "            if \"<<\" not in answer or \">>\" not in answer:\n",
    "                print(f\"Skipping pair with improper calculation format: {question[:30]}...\")\n",
    "                continue\n",
    "                \n",
    "            # Ensure the answer ends with #### format\n",
    "            if not answer.strip().endswith(\"####\"):\n",
    "                # Make sure it has the #### format somewhere\n",
    "                if \"####\" not in answer:\n",
    "                    print(f\"Skipping pair without #### format: {question[:30]}...\")\n",
    "                    continue\n",
    "            \n",
    "            # Add to GSM8K formatted pairs\n",
    "            gsm8k_pairs.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "        \n",
    "        print(f\"Converted {len(gsm8k_pairs)}/{len(qa_pairs)} pairs to GSM8K format\")\n",
    "        return gsm8k_pairs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the pipeline\n",
    "    pipeline = EcommerceQAPairGenerator(\n",
    "        vector_store_path=\"ecommerce_table_rag\",\n",
    "        llm_model=\"llama3\",\n",
    "        output_dir=\"qa_outputs\",\n",
    "        num_questions_per_category=5\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline to generate 20 QA pairs\n",
    "    qa_pairs = pipeline.run_pipeline(num_questions_total=20)\n",
    "    \n",
    "    print(f\"Pipeline execution complete with {len(qa_pairs)} GSM8K-style QA pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7d3baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pipeline components...\n",
      "Successfully loaded vector store from ecommerce_table_rag\n",
      "Starting pipeline to generate 20 total QA pairs\n",
      "Will generate 2 questions per category\n",
      "\n",
      "==================================================\n",
      "Processing category: customer demographics and purchase patterns\n",
      "==================================================\n",
      "Retrieving context for query: 'customer demographics and purchase patterns'\n",
      "Generating 2 questions...\n",
      "Generated 2 questions for category: customer demographics and purchase patterns\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 1/2: What is the average age of customers who purchased office supplies through our mixed channel, and ho...\n",
      "Retrieving context for question: 'What is the average age of customers who purchased...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n",
      "Warning: Answer lacks calculation format, trying to fix\n",
      "Successfully processed and saved QA pair 1\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing question 2/2: What percentage of customers who used discounts for their purchases were also members of our loyalty...\n",
      "Retrieving context for question: 'What percentage of customers who used discounts fo...'\n",
      "Generating answer...\n",
      "Formatting QA pair...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1178015/3551406452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;31m# Run the pipeline to generate 20 QA pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0mqa_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_questions_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;31m# Validate the generated QA pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1178015/3551406452.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, num_questions_total)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                         \u001b[0;31m# Format the QA pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                         \u001b[0mformatted_qa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_qa_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                         \u001b[0;31m# Save intermediate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1178015/3551406452.py\u001b[0m in \u001b[0;36mformat_qa_pair\u001b[0;34m(self, question, answer)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# Generate formatted QA pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Formatting QA pair...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         response = self.llm.invoke(\n\u001b[0m\u001b[1;32m    239\u001b[0m             format_prompt.format(\n\u001b[1;32m    240\u001b[0m                 \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         return (\n\u001b[0;32m--> 385\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    749\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     async def agenerate_prompt(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 )\n\u001b[1;32m    943\u001b[0m             ]\n\u001b[0;32m--> 944\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             output = (\n\u001b[0;32m--> 774\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    775\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_ollama/llms.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             final_chunk = self._stream_with_aggregation(\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_ollama/llms.py\u001b[0m in \u001b[0;36m_stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     ) -> GenerationChunk:\n\u001b[1;32m    271\u001b[0m         \u001b[0mfinal_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstream_resp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_generate_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_resp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 chunk = GenerationChunk(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_ollama/llms.py\u001b[0m in \u001b[0;36m_create_generate_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         yield from self._client.generate(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ollama/_client.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbyte_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m                 \u001b[0mtext_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mraw_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m                     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mraw_stream_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bytes_downloaded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_httpcore_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mShieldCancellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"receive_response_body\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "class EcommerceQAPairGenerator:\n",
    "    \"\"\"\n",
    "    Automated pipeline for generating QA pairs from e-commerce data using RAG.\n",
    "    \n",
    "    This pipeline:\n",
    "    1. Generates analytical questions based on e-commerce data\n",
    "    2. Answers these questions with step-by-step reasoning\n",
    "    3. Formats both into the required format for LLM fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vector_store_path: str = \"ecommerce_table_rag\",\n",
    "        llm_model: str = \"llama3\",\n",
    "        output_dir: str = \"qa_outputs\",\n",
    "        num_questions_per_category: int = 5\n",
    "    ):\n",
    "        \"\"\"Initialize the QA generation pipeline\"\"\"\n",
    "        self.vector_store_path = vector_store_path\n",
    "        self.llm_model = llm_model\n",
    "        self.output_dir = output_dir\n",
    "        self.num_questions_per_category = num_questions_per_category\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize LLM, embeddings, and vector store\"\"\"\n",
    "        print(\"Initializing pipeline components...\")\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = OllamaLLM(model=self.llm_model)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        \n",
    "        # Load vector store\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                self.vector_store_path, \n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "            print(f\"Successfully loaded vector store from {self.vector_store_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_questions(self, query: str, num_questions: int = 5) -> List[str]:\n",
    "        \"\"\"Generate analytical questions based on e-commerce data\"\"\"\n",
    "        # Question generation prompt\n",
    "        question_gen_template = \"\"\"\n",
    "        You are an expert in creating educational questions from e-commerce data analysis.\n",
    "        \n",
    "        Based on the following e-commerce data context, create {num_questions} diverse analytical questions that:\n",
    "        1. Require mathematical reasoning and calculations\n",
    "        2. Involve percentages, averages, or comparative analysis\n",
    "        3. Can be answered based on the information provided\n",
    "        4. Are formatted like real-world business intelligence questions\n",
    "        5. Vary in difficulty (some simple, some complex)\n",
    "        \n",
    "        The questions should be focused on e-commerce analytics like customer segmentation, purchase patterns, \n",
    "        product preferences, and customer behavior.\n",
    "        \n",
    "        CONTEXT INFORMATION:\n",
    "        {context}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        - Generate questions that have definitive numerical answers\n",
    "        - Each question should require analyzing the e-commerce data\n",
    "        - Focus on relationships between different dimensions (gender, age, purchase type, etc.)\n",
    "        - Ensure questions are clearly written and unambiguous\n",
    "        - Make questions require step-by-step reasoning to solve\n",
    "        \n",
    "        FORMAT:\n",
    "        1. Question 1\n",
    "        2. Question 2\n",
    "        (and so on)\n",
    "        \n",
    "        QUESTIONS:\n",
    "        \"\"\"\n",
    "        \n",
    "        question_gen_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"num_questions\"],\n",
    "            template=question_gen_template,\n",
    "        )\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        print(f\"Retrieving context for query: '{query}'\")\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate questions\n",
    "        print(f\"Generating {num_questions} questions...\")\n",
    "        response = self.llm.invoke(\n",
    "            question_gen_prompt.format(\n",
    "                context=context_text,\n",
    "                num_questions=num_questions\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract questions\n",
    "        questions = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and (\n",
    "                line.startswith('Q') or \n",
    "                line.startswith('Question') or \n",
    "                (line[0].isdigit() and '.' in line[:3])\n",
    "            ):\n",
    "                # Clean up the question format\n",
    "                clean_question = line\n",
    "                if line[0].isdigit() and '.' in line[:3]:\n",
    "                    clean_question = line.split('.', 1)[1].strip()\n",
    "                elif line.startswith('Question'):\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        clean_question = parts[1].strip()\n",
    "                \n",
    "                questions.append(clean_question)\n",
    "        \n",
    "        # If extraction failed, try a simpler approach\n",
    "        if not questions:\n",
    "            questions = [line.strip() for line in response.split('\\n') if '?' in line]\n",
    "        \n",
    "        # If still empty, use the raw response\n",
    "        if not questions:\n",
    "            print(\"Warning: Could not extract questions, using raw response\")\n",
    "            return [response.strip()]\n",
    "        \n",
    "        return questions[:num_questions]  # Return only the requested number\n",
    "    \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Answer a question using the e-commerce RAG system\"\"\"\n",
    "        # Answering prompt\n",
    "        answering_template = \"\"\"\n",
    "        You are an expert data analyst specializing in e-commerce analytics.\n",
    "        \n",
    "        QUESTION:\n",
    "        {question}\n",
    "        \n",
    "        Use the following e-commerce data to answer the question:\n",
    "        {context}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        1. Analyze the provided e-commerce data carefully\n",
    "        2. Perform any necessary calculations step-by-step\n",
    "        3. Show your work clearly with appropriate mathematical operations\n",
    "        4. Provide a clear, direct answer to the question\n",
    "        5. Format your answer as a detailed explanation with calculations shown\n",
    "        6. Make sure your calculations are accurate\n",
    "        \n",
    "        YOUR DETAILED ANSWER:\n",
    "        \"\"\"\n",
    "        \n",
    "        answer_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\"],\n",
    "            template=answering_template,\n",
    "        )\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        print(f\"Retrieving context for question: '{question[:50]}...'\")\n",
    "        docs = self.retriever.get_relevant_documents(question)\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"Generating answer...\")\n",
    "        response = self.llm.invoke(\n",
    "            answer_prompt.format(\n",
    "                question=question,\n",
    "                context=context_text\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def format_qa_pair(self, question: str, answer: str) -> Dict[str, str]:\n",
    "        \"\"\"Format a question-answer pair into the required format for training data\"\"\"\n",
    "        # QA formatting prompt\n",
    "        format_template = \"\"\"\n",
    "        You are an expert in creating educational math problems with step-by-step solutions.\n",
    "        \n",
    "        Transform this e-commerce analytics question and answer into a format suitable for fine-tuning language models.\n",
    "        \n",
    "        ORIGINAL QUESTION:\n",
    "        {question}\n",
    "        \n",
    "        ORIGINAL ANSWER:\n",
    "        {answer}\n",
    "        \n",
    "        FORMAT REQUIREMENTS:\n",
    "        1. The question should:\n",
    "           - Include all necessary numerical data\n",
    "           - Be clear and focused on e-commerce analytics\n",
    "           - Be self-contained with all required information\n",
    "        \n",
    "        2. The answer MUST:\n",
    "           - Show each calculation step using EXACTLY this format: \"description = <<calculation=result>>result\"\n",
    "           - End with \"#### [numerical answer]\" where [numerical answer] is just the number\n",
    "        \n",
    "        EXAMPLE:\n",
    "        question: An online store had 240 female customers who used discount codes. If this represents 53.1% of all female customers, how many female customers did not use discount codes?\n",
    "        \n",
    "        answer: First, I'll calculate the total number of female customers.\n",
    "        Total female customers = 240 / 0.531 = <<240/0.531=451.98>>451.98 ≈ 452 customers\n",
    "        \n",
    "        Next, I'll find how many didn't use discounts.\n",
    "        Female customers without discounts = 452 - 240 = <<452-240=212>>212 customers\n",
    "        #### 212\n",
    "        \n",
    "        YOUR FORMATTED QA PAIR (use exactly this format):\n",
    "        question: [your formatted question]\n",
    "        answer: [your formatted step-by-step answer]\n",
    "        \"\"\"\n",
    "        \n",
    "        format_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"answer\"],\n",
    "            template=format_template,\n",
    "        )\n",
    "        \n",
    "        # Generate formatted QA pair\n",
    "        print(\"Formatting QA pair...\")\n",
    "        response = self.llm.invoke(\n",
    "            format_prompt.format(\n",
    "                question=question,\n",
    "                answer=answer\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract question and answer\n",
    "        formatted_qa = {\"question\": \"\", \"answer\": \"\"}\n",
    "        \n",
    "        # Simple parsing approach - find \"question:\" and \"answer:\" markers\n",
    "        lines = response.lower().split('\\n')\n",
    "        question_index = -1\n",
    "        answer_index = -1\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if \"question:\" in line:\n",
    "                question_index = i\n",
    "            if \"answer:\" in line and i > question_index:\n",
    "                answer_index = i\n",
    "                break\n",
    "        \n",
    "        if question_index != -1 and answer_index != -1:\n",
    "            # Extract the question\n",
    "            question_text = lines[question_index].split(\"question:\", 1)[1].strip()\n",
    "            if question_index + 1 < answer_index:\n",
    "                # Multi-line question\n",
    "                for j in range(question_index + 1, answer_index):\n",
    "                    question_text += \" \" + lines[j].strip()\n",
    "            \n",
    "            # Extract the answer\n",
    "            answer_lines = []\n",
    "            answer_start = lines[answer_index].split(\"answer:\", 1)[1].strip()\n",
    "            if answer_start:\n",
    "                answer_lines.append(answer_start)\n",
    "            \n",
    "            # Get the rest of the answer\n",
    "            for j in range(answer_index + 1, len(lines)):\n",
    "                answer_lines.append(lines[j].strip())\n",
    "            \n",
    "            formatted_qa[\"question\"] = question_text\n",
    "            formatted_qa[\"answer\"] = \"\\n\".join(answer_lines)\n",
    "        else:\n",
    "            # Fallback if parsing fails\n",
    "            print(\"Warning: Could not parse formatted QA pair, using fallback method\")\n",
    "            parts = response.split(\"question:\", 1)\n",
    "            if len(parts) > 1:\n",
    "                rest = parts[1].strip()\n",
    "                qa_parts = rest.split(\"answer:\", 1)\n",
    "                if len(qa_parts) > 1:\n",
    "                    formatted_qa[\"question\"] = qa_parts[0].strip()\n",
    "                    formatted_qa[\"answer\"] = qa_parts[1].strip()\n",
    "        \n",
    "        # Validate the formatted QA pair\n",
    "        if not formatted_qa[\"question\"] or not formatted_qa[\"answer\"]:\n",
    "            print(\"Warning: Formatted QA pair is incomplete, using original\")\n",
    "            formatted_qa[\"question\"] = question\n",
    "            formatted_qa[\"answer\"] = \"The answer is #### 100\"  # Generic fallback\n",
    "        \n",
    "        # Check if answer has the required format\n",
    "        if \"<<\" not in formatted_qa[\"answer\"] or \">>\" not in formatted_qa[\"answer\"]:\n",
    "            print(\"Warning: Answer lacks calculation format, trying to fix\")\n",
    "            \n",
    "            # Try to extract calculations from the original answer\n",
    "            import re\n",
    "            calculations = re.findall(r'(\\d+\\.?\\d*)\\s*[+\\-*/]\\s*(\\d+\\.?\\d*)\\s*=\\s*(\\d+\\.?\\d*)', answer)\n",
    "            \n",
    "            if calculations:\n",
    "                fixed_answer_lines = []\n",
    "                for op1, op2, result in calculations:\n",
    "                    # Determine the operation\n",
    "                    if \"+\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"+\"\n",
    "                    elif \"-\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"-\"\n",
    "                    elif \"*\" in answer[answer.find(op1):answer.find(result)] or \"×\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"*\"\n",
    "                    elif \"/\" in answer[answer.find(op1):answer.find(result)] or \"÷\" in answer[answer.find(op1):answer.find(result)]:\n",
    "                        operation = \"/\"\n",
    "                    else:\n",
    "                        operation = \"+\"\n",
    "                    \n",
    "                    fixed_answer_lines.append(f\"Step calculation = {op1} {operation} {op2} = <<{op1}{operation}{op2}={result}>>{result}\")\n",
    "                \n",
    "                # Add final answer\n",
    "                final_result = calculations[-1][2] if calculations else \"100\"\n",
    "                fixed_answer_lines.append(f\"#### {final_result}\")\n",
    "                \n",
    "                formatted_qa[\"answer\"] = \"\\n\".join(fixed_answer_lines)\n",
    "        \n",
    "        # Check if answer has the final answer format\n",
    "        if \"####\" not in formatted_qa[\"answer\"]:\n",
    "            print(\"Warning: Answer lacks final answer format, adding it\")\n",
    "            # Try to find a final number in the answer\n",
    "            import re\n",
    "            final_numbers = re.findall(r'(\\d+\\.?\\d*)', formatted_qa[\"answer\"])\n",
    "            final_result = final_numbers[-1] if final_numbers else \"100\"\n",
    "            formatted_qa[\"answer\"] += f\"\\n#### {final_result}\"\n",
    "        \n",
    "        return formatted_qa\n",
    "    \n",
    "    def run_pipeline(self, num_questions_total: int = 20) -> List[Dict[str, str]]:\n",
    "        \"\"\"Run the complete QA pair generation pipeline\"\"\"\n",
    "        # Define the query categories\n",
    "        categories = [\n",
    "            \"customer demographics and purchase patterns\",\n",
    "            \"discount usage and customer satisfaction\",\n",
    "            \"purchase channel preferences by gender and age\",\n",
    "            \"product categories and spending behavior\",\n",
    "            \"loyalty program analysis\",\n",
    "            \"customer retention and frequency\",\n",
    "            \"device usage and purchase behavior\",\n",
    "            \"social media influence on purchases\"\n",
    "        ]\n",
    "        \n",
    "        all_formatted_qa_pairs = []\n",
    "        questions_per_category = min(self.num_questions_per_category, \n",
    "                                    max(1, num_questions_total // len(categories)))\n",
    "        \n",
    "        print(f\"Starting pipeline to generate {num_questions_total} total QA pairs\")\n",
    "        print(f\"Will generate {questions_per_category} questions per category\")\n",
    "        \n",
    "        try:\n",
    "            # Generate questions for each category\n",
    "            for category in categories:\n",
    "                if len(all_formatted_qa_pairs) >= num_questions_total:\n",
    "                    break\n",
    "                    \n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Processing category: {category}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Generate questions\n",
    "                questions = self.generate_questions(\n",
    "                    query=category, \n",
    "                    num_questions=questions_per_category\n",
    "                )\n",
    "                \n",
    "                print(f\"Generated {len(questions)} questions for category: {category}\")\n",
    "                \n",
    "                # Process each question\n",
    "                for i, question in enumerate(questions):\n",
    "                    if len(all_formatted_qa_pairs) >= num_questions_total:\n",
    "                        break\n",
    "                        \n",
    "                    print(f\"\\n{'-'*50}\")\n",
    "                    print(f\"Processing question {i+1}/{len(questions)}: {question[:100]}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Answer the question\n",
    "                        answer = self.answer_question(question)\n",
    "                        \n",
    "                        # Format the QA pair\n",
    "                        formatted_qa = self.format_qa_pair(question, answer)\n",
    "                        \n",
    "                        # Save intermediate results\n",
    "                        qa_pair = {\n",
    "                            \"original_question\": question,\n",
    "                            \"original_answer\": answer,\n",
    "                            \"formatted_question\": formatted_qa[\"question\"],\n",
    "                            \"formatted_answer\": formatted_qa[\"answer\"]\n",
    "                        }\n",
    "                        \n",
    "                        # Add to results\n",
    "                        all_formatted_qa_pairs.append({\n",
    "                            \"question\": formatted_qa[\"question\"],\n",
    "                            \"answer\": formatted_qa[\"answer\"]\n",
    "                        })\n",
    "                        \n",
    "                        # Save individual QA pair for debugging\n",
    "                        with open(f\"{self.output_dir}/qa_pair_{len(all_formatted_qa_pairs)}.json\", \"w\") as f:\n",
    "                            json.dump(qa_pair, f, indent=2)\n",
    "                        \n",
    "                        print(f\"Successfully processed and saved QA pair {len(all_formatted_qa_pairs)}\")\n",
    "                        \n",
    "                        # Optional: Add a small delay to prevent rate limiting\n",
    "                        time.sleep(0.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing question: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Save all formatted QA pairs\n",
    "            final_output_path = f\"{self.output_dir}/formatted_qa_pairs_final.json\"\n",
    "            with open(final_output_path, \"w\") as f:\n",
    "                json.dump(all_formatted_qa_pairs, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nPipeline complete! Generated {len(all_formatted_qa_pairs)} QA pairs\")\n",
    "            print(f\"Final output saved to: {final_output_path}\")\n",
    "            \n",
    "            return all_formatted_qa_pairs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pipeline: {e}\")\n",
    "            \n",
    "            # Save whatever we've got so far\n",
    "            if all_formatted_qa_pairs:\n",
    "                recovery_path = f\"{self.output_dir}/recovered_qa_pairs.json\"\n",
    "                with open(recovery_path, \"w\") as f:\n",
    "                    json.dump(all_formatted_qa_pairs, f, indent=2)\n",
    "                print(f\"Saved {len(all_formatted_qa_pairs)} recovered QA pairs to: {recovery_path}\")\n",
    "            \n",
    "            raise\n",
    "    \n",
    "    def validate_qa_pairs(self, qa_pairs: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Validate and filter QA pairs to ensure they meet quality standards\"\"\"\n",
    "        valid_qa_pairs = []\n",
    "        \n",
    "        for i, qa_pair in enumerate(qa_pairs):\n",
    "            question = qa_pair.get(\"question\", \"\")\n",
    "            answer = qa_pair.get(\"answer\", \"\")\n",
    "            \n",
    "            # Check for required elements\n",
    "            has_question_mark = \"?\" in question\n",
    "            has_calculation_format = \"<<\" in answer and \">>\" in answer\n",
    "            has_final_answer = \"####\" in answer\n",
    "            \n",
    "            if has_question_mark and has_calculation_format and has_final_answer:\n",
    "                valid_qa_pairs.append(qa_pair)\n",
    "            else:\n",
    "                print(f\"Filtering out QA pair {i+1} due to quality issues\")\n",
    "        \n",
    "        print(f\"Validation complete: {len(valid_qa_pairs)}/{len(qa_pairs)} pairs passed validation\")\n",
    "        \n",
    "        # Save validated pairs\n",
    "        if valid_qa_pairs:\n",
    "            validated_path = f\"{self.output_dir}/validated_qa_pairs.json\"\n",
    "            with open(validated_path, \"w\") as f:\n",
    "                json.dump(valid_qa_pairs, f, indent=2)\n",
    "            print(f\"Saved validated QA pairs to: {validated_path}\")\n",
    "        \n",
    "        return valid_qa_pairs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the pipeline\n",
    "    pipeline = EcommerceQAPairGenerator(\n",
    "        vector_store_path=\"ecommerce_table_rag\",\n",
    "        llm_model=\"llama3\",\n",
    "        output_dir=\"qa_outputs\",\n",
    "        num_questions_per_category=5\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline to generate 20 QA pairs\n",
    "    qa_pairs = pipeline.run_pipeline(num_questions_total=20)\n",
    "    \n",
    "    # Validate the generated QA pairs\n",
    "    validated_pairs = pipeline.validate_qa_pairs(qa_pairs)\n",
    "    \n",
    "    print(f\"Pipeline execution complete with {len(validated_pairs)} valid QA pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c24be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30a8dee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"LD_LIBRARY_PATH\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60a38212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib/libcudnn.so.8\r\n"
     ]
    }
   ],
   "source": [
    "!find /usr/local -name \"libcudnn.so.8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "727c4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7700525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
      "Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.10.0 requires rich, which is not installed.\n",
      "paddlepaddle 2.3.2 requires protobuf<=3.20.0,>=3.1.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "pymilvus 2.2.13 requires grpcio<=1.56.0,>=1.49.1, but you have grpcio 1.60.0 which is incompatible.\n",
      "tensorflow 2.13.1 requires numpy<=1.24.3,>=1.22, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5190826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oolama",
   "language": "python",
   "name": "oolama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
